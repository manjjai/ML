{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPT 5주차\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class simpleNet_2x3:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2, 3)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.w)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss=cross_entropy_error(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47029095 -0.0986776   0.33701501]\n",
      " [-1.02369413 -0.32536875 -0.0458128 ]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet_2x3()\n",
    "print(net.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.20349929 -0.35203843  0.16097749]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9819302478168852\n"
     ]
    }
   ],
   "source": [
    "t = np.array([1, 0, 0])\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.51731789  0.19372872  0.32358918]\n",
      " [-0.77597684  0.29059307  0.48538376]]\n"
     ]
    }
   ],
   "source": [
    "def f(w):\n",
    "    return net.loss(x, t)\n",
    "dw = numerical_gradient(f, net.w)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPT 6주차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. 학습 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['w1'] =  weight_init_std * \\\n",
    "            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['w2'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self, x):\n",
    "        w1, w2 = self.params['w1'], self.params['w2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, w1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, w2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['w1'] = numerical_gradient(loss_w, self.params['w1'])\n",
    "        grads['b1'] = numerical_gradient(loss_w, self.params['b1'])\n",
    "        grads['w2'] = numerical_gradient(loss_w, self.params['w2'])\n",
    "        grads['b2'] = numerical_gradient(loss_w, self.params['b2'])\n",
    "        return grads\n",
    "#출력 존나 안 나와서 확장할 거임\n",
    "def gradient(self, x, t):\n",
    "    w1, w2 = self.params['w1'], self.params['w2']\n",
    "    b1, b2 = self.params['b1'], self.params['b2']\n",
    "    grads = {}\n",
    "    batch_num = x.shape[0]\n",
    "    \n",
    "    #forward\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, w2) +b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    #backward - 역산: 제대로 된 출력을 위한 가중치 조절\n",
    "    dy = (y-t) / batch_num\n",
    "    grads['w2'] = np.dot(z1.T, dy)\n",
    "    grads['b2'] = np.sum(dy, axis=0)\n",
    "    da1 = np.dot(dy, w2.T)\n",
    "    dz1 = sigmoid_grad(a1) * da1\n",
    "    grads['w1'] = np.dot(x.T, dz1)\n",
    "    grads['b1'] = np.sum(dz1, axis=0)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#위 신경망을 mnist에 적용\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasetsub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_mnist\n\u001b[0;32m      3\u001b[0m (x_train, y_train), (x_test, t_test) \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m train_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m iters_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[1;32me:\\Vscode_Jupyter\\인프심\\7주차\\datasetsub\\mnist.py:109\u001b[0m, in \u001b[0;36mload_mnist\u001b[1;34m(normalize, flatten, one_hot_label)\u001b[0m\n\u001b[0;32m    106\u001b[0m     init_mnist()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 109\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_img\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_img\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "#위 신경망을 mnist에 적용\n",
    "from datasetsub.mnist import load_mnist\n",
    "(x_train, y_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "iters_sum = 100\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "for i in range(iters_sum):\n",
    "    grad = network.numerical_gradient(x_train, y_train)\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "    loss = network.loss(x_train, y_train)\n",
    "    train_loss_list.append(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m iters_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m   \u001b[38;5;66;03m#10000번 말고 다른 숫자로 수정정\u001b[39;00m\n",
      "File \u001b[1;32me:\\Vscode_Jupyter\\인프심\\7주차\\datasetsub\\mnist.py:109\u001b[0m, in \u001b[0;36mload_mnist\u001b[1;34m(normalize, flatten, one_hot_label)\u001b[0m\n\u001b[0;32m    106\u001b[0m     init_mnist()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 109\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_img\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_img\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "iters_sum = 10000   #10000번 말고 다른 숫자로 수정\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_sum):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    grad = network.numerical_gradient(x_batch, y_batch)\n",
    "    for key in ('w1', 'b1', 'w2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, y_train)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(train_acc)\n",
    "        print('훈련정확도, 검증정확도: ' + str(train_acc), + ', ' + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\nx = np.arange(len(train_acc_list))\\nplt.plot(x, train_acc_list, label = 'train_acc')\\nplt.plot(x, test_acc_list, label='test acc', linestyle='--')\\nplt.xlabel('epochs')\\nplt.ylabel('accuracy')\\nplt.ylim(0, 1, 0)\\nplt.legend(loc='lower right')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label = 'train_acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1, 0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
