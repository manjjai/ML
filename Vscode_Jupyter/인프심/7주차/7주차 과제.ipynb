{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 연습\n",
    "### 최소 20개 이상의 자료에 대한 좌표를 지정.\n",
    "### 또한 각 10개 이상의 동일 그룹에 대해 0, 1, ~ 등으로 출력 레이블을 지정\n",
    "### 테스트 좌표는 4개 이상 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x_train, t_train]\n",
      "(array([-4.1 , -4.85]), array([3]))\n",
      "(array([-0.94, -0.75]), array([3]))\n",
      "(array([4.48, 4.46]), array([1]))\n",
      "(array([-0.44,  2.85]), array([2]))\n",
      "(array([-1.52,  1.9 ]), array([2]))\n",
      "(array([-3.77,  1.81]), array([2]))\n",
      "(array([-3.86,  3.45]), array([2]))\n",
      "(array([-4.01, -1.  ]), array([3]))\n",
      "(array([-2.43, -3.71]), array([3]))\n",
      "(array([-2.16, -4.75]), array([3]))\n",
      "(array([-0.28, -3.53]), array([3]))\n",
      "(array([ 1.79, -4.46]), array([4]))\n",
      "(array([-2.85,  2.15]), array([2]))\n",
      "(array([3.77, 1.42]), array([1]))\n",
      "(array([-4.64, -4.69]), array([3]))\n",
      "(array([ 0.89, -3.23]), array([4]))\n",
      "(array([0.45, 4.15]), array([1]))\n",
      "(array([3.96, 2.03]), array([1]))\n",
      "(array([ 0.29, -2.6 ]), array([4]))\n",
      "(array([-2.34, -0.11]), array([3]))\n",
      "(array([-1.22,  3.38]), array([2]))\n",
      "(array([-2.29, -4.05]), array([3]))\n",
      "(array([ 1.65, -2.71]), array([4]))\n",
      "(array([-4.45,  3.58]), array([2]))\n",
      "(array([-0.03, -4.38]), array([3]))\n",
      "(array([4.18, 0.68]), array([1]))\n",
      "(array([-1.38,  1.12]), array([2]))\n",
      "(array([-3.75, -0.8 ]), array([3]))\n",
      "(array([-3.87, -4.58]), array([3]))\n",
      "(array([ 3.64, -1.72]), array([4]))\n",
      "(array([-0.27, -2.36]), array([3]))\n",
      "(array([3.46, 1.4 ]), array([1]))\n",
      "(array([3.49, 3.75]), array([1]))\n",
      "(array([0.72, 2.28]), array([1]))\n",
      "(array([ 3.39, -0.92]), array([4]))\n",
      "(array([ 2.87, -1.57]), array([4]))\n",
      "(array([ 1.97, -3.91]), array([4]))\n",
      "(array([-1.07,  4.73]), array([2]))\n",
      "(array([ 1.88, -1.28]), array([4]))\n",
      "(array([-3.57, -0.24]), array([3]))\n",
      "(array([-4.53,  4.14]), array([2]))\n",
      "(array([1.42, 3.59]), array([1]))\n",
      "(array([ 2.81, -3.07]), array([4]))\n",
      "(array([-4.44, -0.77]), array([3]))\n",
      "(array([4.15, 4.12]), array([1]))\n",
      "(array([0.21, 4.4 ]), array([1]))\n",
      "(array([-4.56, -3.93]), array([3]))\n",
      "(array([-3.37, -0.83]), array([3]))\n",
      "(array([ 3.55, -2.56]), array([4]))\n",
      "(array([-3.37, -2.28]), array([3]))\n",
      "(array([ 2.88, -1.93]), array([4]))\n",
      "(array([-3.38, -0.62]), array([3]))\n",
      "(array([-0.27, -0.14]), array([3]))\n",
      "(array([1.63, 3.63]), array([1]))\n",
      "(array([-0.66, -1.71]), array([3]))\n",
      "(array([ 0.62, -3.52]), array([4]))\n",
      "(array([-3.75,  1.03]), array([2]))\n",
      "(array([ 1.13, -0.93]), array([4]))\n",
      "(array([4.94, 4.57]), array([1]))\n",
      "(array([-4.91,  0.06]), array([2]))\n",
      "(array([ 2.07, -2.38]), array([4]))\n",
      "(array([ 4.12, -2.22]), array([4]))\n",
      "(array([ 3.74, -0.17]), array([4]))\n",
      "(array([ 3.47, -3.2 ]), array([4]))\n",
      "(array([2.66, 4.54]), array([1]))\n",
      "(array([1.05, 1.01]), array([1]))\n",
      "(array([-0.79, -2.06]), array([3]))\n",
      "(array([4.21, 2.74]), array([1]))\n",
      "(array([4.84, 4.2 ]), array([1]))\n",
      "(array([3.65, 4.99]), array([1]))\n",
      "(array([-3.24, -2.33]), array([3]))\n",
      "(array([-0.46, -0.73]), array([3]))\n",
      "(array([ 0.08, -4.2 ]), array([4]))\n",
      "(array([-1.35, -2.61]), array([3]))\n",
      "(array([4.03, 0.84]), array([1]))\n",
      "(array([-0.64, -4.32]), array([3]))\n",
      "(array([-4.36,  2.09]), array([2]))\n",
      "(array([-4.3 , -0.42]), array([3]))\n",
      "(array([0.19, 4.21]), array([1]))\n",
      "(array([-0.94,  4.62]), array([2]))\n",
      "(array([-2.72,  1.95]), array([2]))\n",
      "(array([4.44, 2.36]), array([1]))\n",
      "(array([-0.27, -3.45]), array([3]))\n",
      "(array([-0.17,  0.76]), array([2]))\n",
      "(array([4.48, 0.6 ]), array([1]))\n",
      "(array([-0.79, -4.36]), array([3]))\n",
      "(array([-3.11,  1.05]), array([2]))\n",
      "(array([-3.81,  3.17]), array([2]))\n",
      "(array([-4.95, -3.07]), array([3]))\n",
      "(array([1.77, 2.85]), array([1]))\n",
      "(array([-0.37,  2.12]), array([2]))\n",
      "(array([-4.18,  1.92]), array([2]))\n",
      "(array([ 2.69, -3.78]), array([4]))\n",
      "(array([-1.62,  0.78]), array([2]))\n",
      "(array([-0.25, -0.7 ]), array([3]))\n",
      "(array([-2.8 ,  3.76]), array([2]))\n",
      "(array([4.97, 0.62]), array([1]))\n",
      "(array([-2.49,  1.73]), array([2]))\n",
      "(array([-4.88,  3.3 ]), array([2]))\n",
      "(array([-2.11,  2.56]), array([2]))\n",
      "\n",
      "[x_test, t_test]\n",
      "(array([ 1.  , -4.27]), array([4]))\n",
      "(array([-2.68, -4.37]), array([3]))\n",
      "(array([-0.71, -3.72]), array([3]))\n",
      "(array([-2.68, -4.89]), array([3]))\n",
      "(array([-1.55, -2.83]), array([3]))\n",
      "(array([-1.43,  3.27]), array([2]))\n",
      "(array([1.01, 4.46]), array([1]))\n",
      "(array([4.84, 0.69]), array([1]))\n",
      "(array([2.57, 3.68]), array([1]))\n",
      "(array([-0.52,  2.4 ]), array([2]))\n",
      "(array([2.45, 2.32]), array([1]))\n",
      "(array([1.82, 1.41]), array([1]))\n",
      "(array([-1.01,  2.65]), array([2]))\n",
      "(array([-0.58,  1.07]), array([2]))\n",
      "(array([3.39, 0.18]), array([1]))\n",
      "(array([ 4.22, -2.98]), array([4]))\n",
      "(array([0.7 , 1.52]), array([1]))\n",
      "(array([-3.69,  3.84]), array([2]))\n",
      "(array([ 1.74, -3.4 ]), array([4]))\n",
      "(array([4.36, 1.97]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "#제 1, 2, 3, 4사분면 나누기\n",
    "import numpy as np\n",
    "import random\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): #입력 크기, 히든 레이어 크기, 출력 크기, 실수값으로 조지기\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #입력x히든 사이즈로 난수 배열 만들고 실수로 값 변환\n",
    "        self.params['b1'] = np.zeros(hidden_size)   #히든 레이어 개수만큼 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) #np.sum(y==t) y==t인 case의 횟수를 저장, 이를 평균을 내줌\n",
    "        return accuracy                               #그렇게 구한 정확도를 반환\n",
    "    \n",
    "    def numerical_gradient(self, x, t):                 #손실함수를 w에 대해 미분함 -> gradient를 반환\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):   #x=20x2, t=20x1\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        batch_num = x.shape[0]  #20\n",
    "        #forward\n",
    "        a1 = np.dot(x, W1) + b1 \n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        #backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        return grads\n",
    "\n",
    "#학습데이터 20개 이상 생성: 100개\n",
    "train_x = np.random.uniform(-5, 5, (100, 2))\n",
    "train_x = np.round(train_x, decimals=2)\n",
    "\n",
    "# 출력 레이블: 1, 2, 3, 4\n",
    "train_t = np.where((train_x[:, 0] > 0) & (train_x[:, 1] > 0), 1, \n",
    "    np.where((train_x[:, 0] < 0) & (train_x[:, 1] > 0), 2,\n",
    "    np.where((train_x[:, 0] < 0) & (train_x[:, 1] < 0), 3,\n",
    "    np.where((train_x[:, 0] > 0) & (train_x[:, 1] < 0), 4,  \n",
    "    0)))).reshape(-1, 1)\n",
    "\n",
    "# 테스트 데이터 4개 이상: 20\n",
    "test_x = np.random.uniform(-5, 5, (20, 2))\n",
    "test_x = np.round(test_x, decimals=2)\n",
    "\n",
    "# x_test의 예상 레이블 4개\n",
    "test_t = np.where((test_x[:, 0] > 0) & (test_x[:, 1] > 0), 1, \n",
    "    np.where((test_x[:, 0] < 0) & (test_x[:, 1] > 0), 2,\n",
    "    np.where((test_x[:, 0] < 0) & (test_x[:, 1] < 0), 3,\n",
    "    np.where((test_x[:, 0] > 0) & (test_x[:, 1] < 0), 4,  \n",
    "    0)))).reshape(-1, 1)\n",
    "\n",
    "network = TwoLayerNet(2, 3, 1) #(784, 50, 10)-> (2, 5||3, 1), (#입력, #히든 레이어, #출력)\n",
    "iters_num = 10000                 #10000->100\n",
    "train_size = train_x.shape[0] #20\n",
    "learning_rate = 0.01\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = 10\n",
    "for i in range(iters_num):\n",
    "    grad = network.gradient(train_x, train_t)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(train_x, train_t)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(train_x, train_t)\n",
    "        test_acc = network.accuracy(test_x, test_t)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        #print(i, \"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "print(\"[x_train, t_train]\")\n",
    "for j in range(100):\n",
    "    print(f\"{train_x[j], train_t[j]}\")\n",
    "\n",
    "print(\"\\n[x_test, t_test]\")\n",
    "for k in range(20):\n",
    "    print(f\"{test_x[k], test_t[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train acc:0.28, test acc 0.35, loss: 1.387962088058786\n",
      "10 train acc:0.28, test acc 0.35, loss: 1.3789820382115505\n",
      "20 train acc:0.28, test acc 0.35, loss: 1.376181094020486\n",
      "30 train acc:0.28, test acc 0.35, loss: 1.3750776991610314\n",
      "40 train acc:0.28, test acc 0.35, loss: 1.3744719692579508\n",
      "50 train acc:0.28, test acc 0.35, loss: 1.3740149401673005\n",
      "60 train acc:0.28, test acc 0.35, loss: 1.3735993267517792\n",
      "70 train acc:0.28, test acc 0.35, loss: 1.3731904285844394\n",
      "80 train acc:0.28, test acc 0.35, loss: 1.372776122527186\n",
      "90 train acc:0.28, test acc 0.35, loss: 1.3723514536939632\n",
      "100 train acc:0.28, test acc 0.35, loss: 1.3719137565423905\n",
      "110 train acc:0.28, test acc 0.35, loss: 1.371461080506537\n",
      "120 train acc:0.28, test acc 0.35, loss: 1.370991673695684\n",
      "130 train acc:0.28, test acc 0.35, loss: 1.3705038098134887\n",
      "140 train acc:0.28, test acc 0.35, loss: 1.369995727325214\n",
      "150 train acc:0.28, test acc 0.35, loss: 1.3694656054908447\n",
      "160 train acc:0.28, test acc 0.35, loss: 1.3689115524973001\n",
      "170 train acc:0.28, test acc 0.35, loss: 1.3683315975156094\n",
      "180 train acc:0.28, test acc 0.35, loss: 1.3677236839765836\n",
      "190 train acc:0.28, test acc 0.35, loss: 1.367085663167334\n",
      "200 train acc:0.28, test acc 0.35, loss: 1.3664152878517997\n",
      "210 train acc:0.28, test acc 0.35, loss: 1.3657102058196018\n",
      "220 train acc:0.28, test acc 0.35, loss: 1.364967953336305\n",
      "230 train acc:0.28, test acc 0.35, loss: 1.3641859484932422\n",
      "240 train acc:0.28, test acc 0.35, loss: 1.3633614844658397\n",
      "250 train acc:0.28, test acc 0.35, loss: 1.3624917226957287\n",
      "260 train acc:0.28, test acc 0.35, loss: 1.3615736860170853\n",
      "270 train acc:0.28, test acc 0.35, loss: 1.360604251752771\n",
      "280 train acc:0.28, test acc 0.35, loss: 1.3595801448113098\n",
      "290 train acc:0.28, test acc 0.35, loss: 1.3584979308217249\n",
      "300 train acc:0.28, test acc 0.35, loss: 1.3573540093498229\n",
      "310 train acc:0.31, test acc 0.35, loss: 1.356144607246723\n",
      "320 train acc:0.32, test acc 0.35, loss: 1.3548657721882804\n",
      "330 train acc:0.33, test acc 0.35, loss: 1.3535133664726116\n",
      "340 train acc:0.35, test acc 0.35, loss: 1.3520830611521755\n",
      "350 train acc:0.35, test acc 0.35, loss: 1.350570330586792\n",
      "360 train acc:0.36, test acc 0.35, loss: 1.3489704475146136\n",
      "370 train acc:0.38, test acc 0.35, loss: 1.3472784787493064\n",
      "380 train acc:0.39, test acc 0.35, loss: 1.3454892816235509\n",
      "390 train acc:0.41, test acc 0.35, loss: 1.343597501311298\n",
      "400 train acc:0.43, test acc 0.4, loss: 1.341597569173941\n",
      "410 train acc:0.46, test acc 0.4, loss: 1.3394837022884891\n",
      "420 train acc:0.49, test acc 0.4, loss: 1.3372499043287949\n",
      "430 train acc:0.49, test acc 0.4, loss: 1.334889967983614\n",
      "440 train acc:0.49, test acc 0.4, loss: 1.332397479107493\n",
      "450 train acc:0.5, test acc 0.4, loss: 1.3297658228117777\n",
      "460 train acc:0.5, test acc 0.4, loss: 1.3269881917130615\n",
      "470 train acc:0.53, test acc 0.45, loss: 1.3240575965645724\n",
      "480 train acc:0.54, test acc 0.45, loss: 1.3209668795018232\n",
      "490 train acc:0.56, test acc 0.5, loss: 1.3177087301366808\n",
      "500 train acc:0.57, test acc 0.5, loss: 1.3142757047331184\n",
      "510 train acc:0.59, test acc 0.5, loss: 1.31066024869258\n",
      "520 train acc:0.59, test acc 0.5, loss: 1.3068547225662817\n",
      "530 train acc:0.6, test acc 0.5, loss: 1.3028514317950417\n",
      "540 train acc:0.6, test acc 0.5, loss: 1.2986426603535692\n",
      "550 train acc:0.61, test acc 0.5, loss: 1.294220708444661\n",
      "560 train acc:0.63, test acc 0.5, loss: 1.2895779343487022\n",
      "570 train acc:0.64, test acc 0.5, loss: 1.2847068004845807\n",
      "580 train acc:0.66, test acc 0.55, loss: 1.279599923679026\n",
      "590 train acc:0.67, test acc 0.55, loss: 1.274250129572179\n",
      "600 train acc:0.68, test acc 0.55, loss: 1.2686505110078214\n",
      "610 train acc:0.68, test acc 0.55, loss: 1.262794490167428\n",
      "620 train acc:0.7, test acc 0.6, loss: 1.2566758841087076\n",
      "630 train acc:0.7, test acc 0.6, loss: 1.2502889732627478\n",
      "640 train acc:0.7, test acc 0.6, loss: 1.2436285723309164\n",
      "650 train acc:0.72, test acc 0.65, loss: 1.2366901029055386\n",
      "660 train acc:0.72, test acc 0.65, loss: 1.2294696670198528\n",
      "670 train acc:0.72, test acc 0.65, loss: 1.2219641207162528\n",
      "680 train acc:0.72, test acc 0.65, loss: 1.2141711466112501\n",
      "690 train acc:0.73, test acc 0.65, loss: 1.206089324335291\n",
      "700 train acc:0.74, test acc 0.65, loss: 1.197718197640267\n",
      "710 train acc:0.74, test acc 0.65, loss: 1.1890583369021104\n",
      "720 train acc:0.75, test acc 0.65, loss: 1.180111395705084\n",
      "730 train acc:0.75, test acc 0.65, loss: 1.170880160182843\n",
      "740 train acc:0.75, test acc 0.65, loss: 1.1613685898129857\n",
      "750 train acc:0.75, test acc 0.65, loss: 1.1515818484198528\n",
      "760 train acc:0.76, test acc 0.65, loss: 1.1415263242367801\n",
      "770 train acc:0.76, test acc 0.65, loss: 1.131209638014594\n",
      "780 train acc:0.77, test acc 0.65, loss: 1.1206406383369925\n",
      "790 train acc:0.82, test acc 0.65, loss: 1.1098293835131043\n",
      "800 train acc:0.81, test acc 0.65, loss: 1.098787109658698\n",
      "810 train acc:0.82, test acc 0.65, loss: 1.087526184844387\n",
      "820 train acc:0.82, test acc 0.65, loss: 1.0760600494743726\n",
      "830 train acc:0.82, test acc 0.7, loss: 1.064403143354243\n",
      "840 train acc:0.82, test acc 0.7, loss: 1.0525708202016046\n",
      "850 train acc:0.82, test acc 0.7, loss: 1.0405792506391003\n",
      "860 train acc:0.82, test acc 0.7, loss: 1.0284453149756452\n",
      "870 train acc:0.82, test acc 0.7, loss: 1.016186487319257\n",
      "880 train acc:0.83, test acc 0.7, loss: 1.0038207127650969\n",
      "890 train acc:0.83, test acc 0.7, loss: 0.991366279558245\n",
      "900 train acc:0.83, test acc 0.7, loss: 0.9788416882368132\n",
      "910 train acc:0.84, test acc 0.7, loss: 0.9662655198137453\n",
      "920 train acc:0.85, test acc 0.7, loss: 0.9536563050535932\n",
      "930 train acc:0.85, test acc 0.7, loss: 0.9410323968443537\n",
      "940 train acc:0.86, test acc 0.7, loss: 0.9284118475567575\n",
      "950 train acc:0.86, test acc 0.7, loss: 0.9158122931288823\n",
      "960 train acc:0.86, test acc 0.7, loss: 0.9032508454188186\n",
      "970 train acc:0.87, test acc 0.7, loss: 0.8907439941400217\n",
      "980 train acc:0.87, test acc 0.7, loss: 0.8783075194414974\n",
      "990 train acc:0.88, test acc 0.7, loss: 0.8659564159273065\n",
      "1000 train acc:0.9, test acc 0.7, loss: 0.8537048286363315\n",
      "1010 train acc:0.9, test acc 0.7, loss: 0.8415660012329176\n",
      "1020 train acc:0.9, test acc 0.7, loss: 0.829552236400234\n",
      "1030 train acc:0.9, test acc 0.7, loss: 0.8176748681884256\n",
      "1040 train acc:0.9, test acc 0.75, loss: 0.805944245854917\n",
      "1050 train acc:0.91, test acc 0.8, loss: 0.794369728549261\n",
      "1060 train acc:0.91, test acc 0.8, loss: 0.7829596900427893\n",
      "1070 train acc:0.91, test acc 0.8, loss: 0.7717215325855293\n",
      "1080 train acc:0.91, test acc 0.8, loss: 0.7606617088895418\n",
      "1090 train acc:0.92, test acc 0.8, loss: 0.7497857511876661\n",
      "1100 train acc:0.92, test acc 0.85, loss: 0.7390983062973925\n",
      "1110 train acc:0.92, test acc 0.85, loss: 0.7286031756279244\n",
      "1120 train acc:0.92, test acc 0.85, loss: 0.7183033591007165\n",
      "1130 train acc:0.92, test acc 0.9, loss: 0.7082011020056775\n",
      "1140 train acc:0.92, test acc 0.9, loss: 0.6982979438826155\n",
      "1150 train acc:0.93, test acc 0.9, loss: 0.6885947685961663\n",
      "1160 train acc:0.93, test acc 0.9, loss: 0.6790918548585304\n",
      "1170 train acc:0.93, test acc 0.95, loss: 0.6697889265442933\n",
      "1180 train acc:0.93, test acc 0.95, loss: 0.6606852022323784\n",
      "1190 train acc:0.93, test acc 0.95, loss: 0.6517794434992144\n",
      "1200 train acc:0.93, test acc 0.95, loss: 0.6430700015724267\n",
      "1210 train acc:0.93, test acc 0.95, loss: 0.6345548620342513\n",
      "1220 train acc:0.93, test acc 0.95, loss: 0.6262316873372886\n",
      "1230 train acc:0.94, test acc 0.95, loss: 0.6180978569615067\n",
      "1240 train acc:0.94, test acc 0.95, loss: 0.6101505051001772\n",
      "1250 train acc:0.94, test acc 0.95, loss: 0.6023865558136863\n",
      "1260 train acc:0.94, test acc 0.95, loss: 0.5948027556340215\n",
      "1270 train acc:0.94, test acc 0.95, loss: 0.587395703639595\n",
      "1280 train acc:0.94, test acc 0.95, loss: 0.5801618790503711\n",
      "1290 train acc:0.94, test acc 0.95, loss: 0.5730976664175926\n",
      "1300 train acc:0.94, test acc 0.95, loss: 0.5661993785013381\n",
      "1310 train acc:0.94, test acc 0.95, loss: 0.5594632769433275\n",
      "1320 train acc:0.94, test acc 0.95, loss: 0.5528855908524278\n",
      "1330 train acc:0.95, test acc 0.95, loss: 0.5464625334267792\n",
      "1340 train acc:0.95, test acc 0.95, loss: 0.540190316739936\n",
      "1350 train acc:0.95, test acc 0.95, loss: 0.5340651648193828\n",
      "1360 train acc:0.95, test acc 0.95, loss: 0.5280833251447283\n",
      "1370 train acc:0.95, test acc 0.95, loss: 0.5222410786901788\n",
      "1380 train acc:0.95, test acc 0.95, loss: 0.5165347486319426\n",
      "1390 train acc:0.95, test acc 0.95, loss: 0.5109607078363037\n",
      "1400 train acc:0.95, test acc 0.95, loss: 0.5055153852384996\n",
      "1410 train acc:0.95, test acc 0.95, loss: 0.5001952712164806\n",
      "1420 train acc:0.95, test acc 0.95, loss: 0.49499692205728524\n",
      "1430 train acc:0.95, test acc 0.95, loss: 0.4899169636073117\n",
      "1440 train acc:0.95, test acc 0.95, loss: 0.48495209419129925\n",
      "1450 train acc:0.95, test acc 0.95, loss: 0.48009908687847974\n",
      "1460 train acc:0.95, test acc 0.95, loss: 0.47535479116817414\n",
      "1470 train acc:0.95, test acc 0.95, loss: 0.4707161341611505\n",
      "1480 train acc:0.95, test acc 0.95, loss: 0.4661801212773888\n",
      "1490 train acc:0.95, test acc 0.95, loss: 0.4617438365755108\n",
      "1500 train acc:0.95, test acc 0.95, loss: 0.4574044427240764\n",
      "1510 train acc:0.95, test acc 0.95, loss: 0.4531591806702094\n",
      "1520 train acc:0.95, test acc 0.95, loss: 0.4490053690466082\n",
      "1530 train acc:0.95, test acc 0.95, loss: 0.4449404033539103\n",
      "1540 train acc:0.95, test acc 0.95, loss: 0.4409617549516105\n",
      "1550 train acc:0.95, test acc 0.95, loss: 0.4370669698872615\n",
      "1560 train acc:0.95, test acc 0.95, loss: 0.4332536675905103\n",
      "1570 train acc:0.95, test acc 0.95, loss: 0.4295195394556204\n",
      "1580 train acc:0.95, test acc 0.95, loss: 0.42586234733347955\n",
      "1590 train acc:0.95, test acc 0.95, loss: 0.4222799219516951\n",
      "1600 train acc:0.95, test acc 0.95, loss: 0.4187701612791963\n",
      "1610 train acc:0.95, test acc 0.95, loss: 0.4153310288497942\n",
      "1620 train acc:0.95, test acc 0.95, loss: 0.41196055205737286\n",
      "1630 train acc:0.95, test acc 0.95, loss: 0.40865682043378365\n",
      "1640 train acc:0.96, test acc 0.95, loss: 0.40541798391908224\n",
      "1650 train acc:0.96, test acc 0.95, loss: 0.40224225113245465\n",
      "1660 train acc:0.96, test acc 0.95, loss: 0.3991278876510278\n",
      "1670 train acc:0.96, test acc 0.95, loss: 0.3960732143027342\n",
      "1680 train acc:0.97, test acc 0.95, loss: 0.393076605478478\n",
      "1690 train acc:0.97, test acc 0.95, loss: 0.39013648746804186\n",
      "1700 train acc:0.97, test acc 0.95, loss: 0.3872513368234426\n",
      "1710 train acc:0.97, test acc 0.95, loss: 0.38441967875281235\n",
      "1720 train acc:0.97, test acc 0.95, loss: 0.38164008554730855\n",
      "1730 train acc:0.97, test acc 0.95, loss: 0.37891117504306465\n",
      "1740 train acc:0.97, test acc 0.95, loss: 0.37623160911974923\n",
      "1750 train acc:0.97, test acc 0.95, loss: 0.3736000922369234\n",
      "1760 train acc:0.97, test acc 0.95, loss: 0.3710153700090476\n",
      "1770 train acc:0.97, test acc 0.95, loss: 0.36847622781969724\n",
      "1780 train acc:0.97, test acc 0.95, loss: 0.3659814894752939\n",
      "1790 train acc:0.97, test acc 0.95, loss: 0.3635300158984402\n",
      "1800 train acc:0.97, test acc 0.95, loss: 0.36112070386075873\n",
      "1810 train acc:0.97, test acc 0.95, loss: 0.35875248475496974\n",
      "1820 train acc:0.97, test acc 0.95, loss: 0.35642432340581304\n",
      "1830 train acc:0.97, test acc 0.95, loss: 0.3541352169192969\n",
      "1840 train acc:0.97, test acc 0.95, loss: 0.35188419356966444\n",
      "1850 train acc:0.97, test acc 0.95, loss: 0.34967031172338436\n",
      "1860 train acc:0.97, test acc 0.95, loss: 0.3474926587994113\n",
      "1870 train acc:0.97, test acc 0.95, loss: 0.34535035026490485\n",
      "1880 train acc:0.97, test acc 0.95, loss: 0.3432425286655563\n",
      "1890 train acc:0.97, test acc 0.95, loss: 0.3411683626896428\n",
      "1900 train acc:0.97, test acc 0.95, loss: 0.33912704626490153\n",
      "1910 train acc:0.97, test acc 0.95, loss: 0.33711779768730443\n",
      "1920 train acc:0.97, test acc 0.95, loss: 0.335139858780803\n",
      "1930 train acc:0.97, test acc 0.95, loss: 0.3331924940871116\n",
      "1940 train acc:0.97, test acc 0.95, loss: 0.3312749900845967\n",
      "1950 train acc:0.97, test acc 0.95, loss: 0.32938665443534554\n",
      "1960 train acc:0.97, test acc 0.95, loss: 0.3275268152594982\n",
      "1970 train acc:0.97, test acc 0.95, loss: 0.3256948204359371\n",
      "1980 train acc:0.97, test acc 0.95, loss: 0.323890036928442\n",
      "1990 train acc:0.97, test acc 0.95, loss: 0.3221118501364391\n",
      "2000 train acc:0.97, test acc 0.95, loss: 0.3203596632694834\n",
      "2010 train acc:0.97, test acc 0.95, loss: 0.31863289674464024\n",
      "2020 train acc:0.97, test acc 0.95, loss: 0.31693098760594773\n",
      "2030 train acc:0.97, test acc 0.95, loss: 0.315253388965165\n",
      "2040 train acc:0.97, test acc 0.95, loss: 0.31359956946303175\n",
      "2050 train acc:0.97, test acc 0.95, loss: 0.3119690127502874\n",
      "2060 train acc:0.97, test acc 0.95, loss: 0.31036121698772157\n",
      "2070 train acc:0.97, test acc 0.95, loss: 0.3087756943645467\n",
      "2080 train acc:0.97, test acc 0.95, loss: 0.30721197063441086\n",
      "2090 train acc:0.97, test acc 0.95, loss: 0.30566958466838656\n",
      "2100 train acc:0.97, test acc 0.95, loss: 0.30414808802429666\n",
      "2110 train acc:0.97, test acc 0.95, loss: 0.30264704453175895\n",
      "2120 train acc:0.97, test acc 0.95, loss: 0.3011660298923514\n",
      "2130 train acc:0.97, test acc 0.95, loss: 0.2997046312943254\n",
      "2140 train acc:0.97, test acc 0.95, loss: 0.29826244704130667\n",
      "2150 train acc:0.97, test acc 0.95, loss: 0.2968390861944555\n",
      "2160 train acc:0.97, test acc 0.95, loss: 0.29543416822756413\n",
      "2170 train acc:0.97, test acc 0.95, loss: 0.2940473226946008\n",
      "2180 train acc:0.97, test acc 0.95, loss: 0.2926781889092192\n",
      "2190 train acc:0.97, test acc 0.95, loss: 0.2913264156357743\n",
      "2200 train acc:0.97, test acc 0.95, loss: 0.2899916607914044\n",
      "2210 train acc:0.97, test acc 0.95, loss: 0.2886735911587522\n",
      "2220 train acc:0.97, test acc 0.95, loss: 0.2873718821089158\n",
      "2230 train acc:0.97, test acc 0.95, loss: 0.28608621733423883\n",
      "2240 train acc:0.97, test acc 0.95, loss: 0.28481628859056085\n",
      "2250 train acc:0.97, test acc 0.95, loss: 0.2835617954485643\n",
      "2260 train acc:0.97, test acc 0.95, loss: 0.28232244505387044\n",
      "2270 train acc:0.97, test acc 0.95, loss: 0.2810979518955504\n",
      "2280 train acc:0.97, test acc 0.95, loss: 0.27988803758272673\n",
      "2290 train acc:0.97, test acc 0.95, loss: 0.27869243062895954\n",
      "2300 train acc:0.97, test acc 0.95, loss: 0.27751086624411875\n",
      "2310 train acc:0.97, test acc 0.95, loss: 0.2763430861334586\n",
      "2320 train acc:0.97, test acc 0.95, loss: 0.27518883830361957\n",
      "2330 train acc:0.97, test acc 0.95, loss: 0.2740478768752974\n",
      "2340 train acc:0.97, test acc 0.95, loss: 0.27291996190232387\n",
      "2350 train acc:0.96, test acc 0.95, loss: 0.2718048591969201\n",
      "2360 train acc:0.96, test acc 0.95, loss: 0.2707023401608886\n",
      "2370 train acc:0.96, test acc 0.95, loss: 0.26961218162252065\n",
      "2380 train acc:0.96, test acc 0.95, loss: 0.2685341656790055\n",
      "2390 train acc:0.96, test acc 0.95, loss: 0.2674680795441356\n",
      "2400 train acc:0.96, test acc 0.95, loss: 0.26641371540110925\n",
      "2410 train acc:0.96, test acc 0.95, loss: 0.2653708702602434\n",
      "2420 train acc:0.96, test acc 0.95, loss: 0.2643393458214113\n",
      "2430 train acc:0.96, test acc 0.95, loss: 0.26331894834103303\n",
      "2440 train acc:0.96, test acc 0.95, loss: 0.2623094885034496\n",
      "2450 train acc:0.96, test acc 0.95, loss: 0.2613107812965195\n",
      "2460 train acc:0.96, test acc 0.95, loss: 0.2603226458912834\n",
      "2470 train acc:0.96, test acc 0.95, loss: 0.2593449055255469\n",
      "2480 train acc:0.96, test acc 0.95, loss: 0.25837738739124133\n",
      "2490 train acc:0.96, test acc 0.95, loss: 0.25741992252542156\n",
      "2500 train acc:0.96, test acc 0.95, loss: 0.2564723457047724\n",
      "2510 train acc:0.96, test acc 0.95, loss: 0.25553449534349476\n",
      "2520 train acc:0.96, test acc 0.95, loss: 0.254606213394452\n",
      "2530 train acc:0.96, test acc 0.95, loss: 0.2536873452534563\n",
      "2540 train acc:0.96, test acc 0.95, loss: 0.25277773966658673\n",
      "2550 train acc:0.96, test acc 0.95, loss: 0.251877248640428\n",
      "2560 train acc:0.96, test acc 0.95, loss: 0.2509857273551275\n",
      "2570 train acc:0.96, test acc 0.95, loss: 0.2501030340801704\n",
      "2580 train acc:0.96, test acc 0.95, loss: 0.24922903009277786\n",
      "2590 train acc:0.96, test acc 0.95, loss: 0.24836357959883432\n",
      "2600 train acc:0.96, test acc 0.95, loss: 0.2475065496562575\n",
      "2610 train acc:0.96, test acc 0.95, loss: 0.2466578101007243\n",
      "2620 train acc:0.97, test acc 0.95, loss: 0.2458172334736707\n",
      "2630 train acc:0.97, test acc 0.95, loss: 0.24498469495248806\n",
      "2640 train acc:0.97, test acc 0.95, loss: 0.24416007228283898\n",
      "2650 train acc:0.97, test acc 0.95, loss: 0.24334324571301946\n",
      "2660 train acc:0.97, test acc 0.95, loss: 0.24253409793029823\n",
      "2670 train acc:0.97, test acc 0.95, loss: 0.241732513999165\n",
      "2680 train acc:0.97, test acc 0.95, loss: 0.2409383813014231\n",
      "2690 train acc:0.97, test acc 0.95, loss: 0.24015158947806298\n",
      "2700 train acc:0.97, test acc 0.95, loss: 0.23937203037285804\n",
      "2710 train acc:0.97, test acc 0.95, loss: 0.23859959797762292\n",
      "2720 train acc:0.97, test acc 0.95, loss: 0.2378341883790805\n",
      "2730 train acc:0.97, test acc 0.95, loss: 0.2370756997072812\n",
      "2740 train acc:0.98, test acc 0.95, loss: 0.236324032085526\n",
      "2750 train acc:0.98, test acc 0.95, loss: 0.23557908758174084\n",
      "2760 train acc:0.98, test acc 0.95, loss: 0.23484077016125532\n",
      "2770 train acc:0.98, test acc 0.95, loss: 0.23410898564094018\n",
      "2780 train acc:0.98, test acc 0.95, loss: 0.2333836416446567\n",
      "2790 train acc:0.98, test acc 0.95, loss: 0.2326646475599786\n",
      "2800 train acc:0.98, test acc 0.95, loss: 0.23195191449614125\n",
      "2810 train acc:0.98, test acc 0.95, loss: 0.23124535524318174\n",
      "2820 train acc:0.98, test acc 0.95, loss: 0.23054488423222846\n",
      "2830 train acc:0.98, test acc 0.95, loss: 0.22985041749690546\n",
      "2840 train acc:0.98, test acc 0.95, loss: 0.2291618726358139\n",
      "2850 train acc:0.98, test acc 0.95, loss: 0.22847916877605812\n",
      "2860 train acc:0.98, test acc 0.95, loss: 0.22780222653778104\n",
      "2870 train acc:0.98, test acc 0.95, loss: 0.22713096799967858\n",
      "2880 train acc:0.98, test acc 0.95, loss: 0.22646531666546113\n",
      "2890 train acc:0.98, test acc 0.95, loss: 0.22580519743123253\n",
      "2900 train acc:0.98, test acc 0.95, loss: 0.22515053655375805\n",
      "2910 train acc:0.98, test acc 0.95, loss: 0.224501261619593\n",
      "2920 train acc:0.98, test acc 0.95, loss: 0.22385730151504543\n",
      "2930 train acc:0.98, test acc 0.95, loss: 0.22321858639694853\n",
      "2940 train acc:0.98, test acc 0.95, loss: 0.22258504766421347\n",
      "2950 train acc:0.98, test acc 0.95, loss: 0.22195661793014473\n",
      "2960 train acc:0.98, test acc 0.95, loss: 0.22133323099548893\n",
      "2970 train acc:0.98, test acc 0.95, loss: 0.22071482182219826\n",
      "2980 train acc:0.98, test acc 0.95, loss: 0.22010132650788536\n",
      "2990 train acc:0.98, test acc 0.95, loss: 0.2194926822609485\n",
      "3000 train acc:0.98, test acc 0.95, loss: 0.21888882737634838\n",
      "3010 train acc:0.98, test acc 0.95, loss: 0.21828970121201444\n",
      "3020 train acc:0.98, test acc 0.95, loss: 0.21769524416586386\n",
      "3030 train acc:0.98, test acc 0.95, loss: 0.21710539765341447\n",
      "3040 train acc:0.98, test acc 0.95, loss: 0.21652010408597291\n",
      "3050 train acc:0.98, test acc 0.95, loss: 0.21593930684938265\n",
      "3060 train acc:0.98, test acc 0.95, loss: 0.2153629502833136\n",
      "3070 train acc:0.98, test acc 0.95, loss: 0.21479097966107857\n",
      "3080 train acc:0.98, test acc 0.95, loss: 0.21422334116996086\n",
      "3090 train acc:0.98, test acc 0.95, loss: 0.21365998189203758\n",
      "3100 train acc:0.98, test acc 0.95, loss: 0.213100849785485\n",
      "3110 train acc:0.98, test acc 0.95, loss: 0.21254589366635096\n",
      "3120 train acc:0.98, test acc 0.95, loss: 0.21199506319078282\n",
      "3130 train acc:0.98, test acc 0.95, loss: 0.21144830883769403\n",
      "3140 train acc:0.98, test acc 0.95, loss: 0.21090558189186218\n",
      "3150 train acc:0.98, test acc 0.95, loss: 0.21036683442744075\n",
      "3160 train acc:0.98, test acc 0.95, loss: 0.20983201929187645\n",
      "3170 train acc:0.98, test acc 0.95, loss: 0.20930109009021944\n",
      "3180 train acc:0.98, test acc 0.95, loss: 0.2087740011698146\n",
      "3190 train acc:0.98, test acc 0.95, loss: 0.2082507076053645\n",
      "3200 train acc:0.98, test acc 0.95, loss: 0.20773116518435267\n",
      "3210 train acc:0.98, test acc 0.95, loss: 0.20721533039281773\n",
      "3220 train acc:0.98, test acc 0.95, loss: 0.20670316040146777\n",
      "3230 train acc:0.98, test acc 0.95, loss: 0.2061946130521263\n",
      "3240 train acc:0.98, test acc 0.95, loss: 0.2056896468445011\n",
      "3250 train acc:0.98, test acc 0.95, loss: 0.20518822092326455\n",
      "3260 train acc:0.98, test acc 0.95, loss: 0.20469029506544067\n",
      "3270 train acc:0.98, test acc 0.95, loss: 0.20419582966808705\n",
      "3280 train acc:0.98, test acc 0.95, loss: 0.203704785736266\n",
      "3290 train acc:0.98, test acc 0.95, loss: 0.2032171248712954\n",
      "3300 train acc:0.98, test acc 0.95, loss: 0.20273280925927298\n",
      "3310 train acc:0.98, test acc 0.95, loss: 0.20225180165986603\n",
      "3320 train acc:0.98, test acc 0.95, loss: 0.20177406539535891\n",
      "3330 train acc:0.98, test acc 0.95, loss: 0.201299564339953\n",
      "3340 train acc:0.98, test acc 0.95, loss: 0.20082826290931052\n",
      "3350 train acc:0.98, test acc 0.95, loss: 0.200360126050337\n",
      "3360 train acc:0.98, test acc 0.95, loss: 0.19989511923119607\n",
      "3370 train acc:0.98, test acc 0.95, loss: 0.199433208431549\n",
      "3380 train acc:0.98, test acc 0.95, loss: 0.19897436013301564\n",
      "3390 train acc:0.98, test acc 0.95, loss: 0.19851854130984758\n",
      "3400 train acc:0.98, test acc 0.95, loss: 0.1980657194198123\n",
      "3410 train acc:0.98, test acc 0.95, loss: 0.19761586239527773\n",
      "3420 train acc:0.98, test acc 0.95, loss: 0.19716893863449694\n",
      "3430 train acc:0.98, test acc 0.95, loss: 0.19672491699308436\n",
      "3440 train acc:0.98, test acc 0.95, loss: 0.19628376677567985\n",
      "3450 train acc:0.98, test acc 0.95, loss: 0.1958454577277964\n",
      "3460 train acc:0.98, test acc 0.95, loss: 0.19540996002784536\n",
      "3470 train acc:0.98, test acc 0.95, loss: 0.1949772442793357\n",
      "3480 train acc:0.98, test acc 0.95, loss: 0.19454728150324305\n",
      "3490 train acc:0.98, test acc 0.95, loss: 0.194120043130543\n",
      "3500 train acc:0.98, test acc 0.95, loss: 0.19369550099490607\n",
      "3510 train acc:0.98, test acc 0.95, loss: 0.19327362732554895\n",
      "3520 train acc:0.98, test acc 0.95, loss: 0.19285439474023897\n",
      "3530 train acc:0.98, test acc 0.95, loss: 0.19243777623844768\n",
      "3540 train acc:0.98, test acc 0.95, loss: 0.19202374519465\n",
      "3550 train acc:0.98, test acc 0.95, loss: 0.1916122753517645\n",
      "3560 train acc:0.98, test acc 0.95, loss: 0.191203340814733\n",
      "3570 train acc:0.98, test acc 0.95, loss: 0.1907969160442345\n",
      "3580 train acc:0.98, test acc 0.95, loss: 0.1903929758505303\n",
      "3590 train acc:0.98, test acc 0.95, loss: 0.18999149538743867\n",
      "3600 train acc:0.98, test acc 0.95, loss: 0.18959245014643383\n",
      "3610 train acc:0.98, test acc 0.95, loss: 0.18919581595086707\n",
      "3620 train acc:0.98, test acc 0.95, loss: 0.18880156895030847\n",
      "3630 train acc:0.98, test acc 0.95, loss: 0.18840968561500293\n",
      "3640 train acc:0.98, test acc 0.95, loss: 0.18802014273044176\n",
      "3650 train acc:0.98, test acc 0.95, loss: 0.18763291739204402\n",
      "3660 train acc:0.98, test acc 0.95, loss: 0.1872479869999462\n",
      "3670 train acc:0.98, test acc 0.95, loss: 0.18686532925389834\n",
      "3680 train acc:0.98, test acc 0.95, loss: 0.18648492214826212\n",
      "3690 train acc:0.98, test acc 0.95, loss: 0.18610674396711047\n",
      "3700 train acc:0.98, test acc 0.95, loss: 0.18573077327942586\n",
      "3710 train acc:0.98, test acc 0.95, loss: 0.1853569889343933\n",
      "3720 train acc:0.98, test acc 0.95, loss: 0.18498537005678845\n",
      "3730 train acc:0.98, test acc 0.95, loss: 0.18461589604245687\n",
      "3740 train acc:0.98, test acc 0.95, loss: 0.18424854655388262\n",
      "3750 train acc:0.98, test acc 0.95, loss: 0.1838833015158446\n",
      "3760 train acc:0.98, test acc 0.95, loss: 0.18352014111115758\n",
      "3770 train acc:0.98, test acc 0.95, loss: 0.18315904577649816\n",
      "3780 train acc:0.98, test acc 0.95, loss: 0.18279999619831\n",
      "3790 train acc:0.98, test acc 0.95, loss: 0.1824429733087913\n",
      "3800 train acc:0.98, test acc 0.95, loss: 0.18208795828195773\n",
      "3810 train acc:0.98, test acc 0.95, loss: 0.18173493252978318\n",
      "3820 train acc:0.98, test acc 0.95, loss: 0.18138387769841405\n",
      "3830 train acc:0.98, test acc 0.95, loss: 0.1810347756644563\n",
      "3840 train acc:0.98, test acc 0.95, loss: 0.18068760853133367\n",
      "3850 train acc:0.98, test acc 0.95, loss: 0.18034235862571527\n",
      "3860 train acc:0.98, test acc 0.95, loss: 0.1799990084940114\n",
      "3870 train acc:0.98, test acc 0.95, loss: 0.17965754089893524\n",
      "3880 train acc:0.98, test acc 0.95, loss: 0.17931793881612967\n",
      "3890 train acc:0.98, test acc 0.95, loss: 0.17898018543085809\n",
      "3900 train acc:0.98, test acc 0.95, loss: 0.17864426413475618\n",
      "3910 train acc:0.99, test acc 0.95, loss: 0.17831015852264545\n",
      "3920 train acc:0.99, test acc 0.95, loss: 0.17797785238940605\n",
      "3930 train acc:0.99, test acc 0.95, loss: 0.17764732972690703\n",
      "3940 train acc:0.99, test acc 0.95, loss: 0.17731857472099363\n",
      "3950 train acc:0.99, test acc 0.95, loss: 0.17699157174853133\n",
      "3960 train acc:0.99, test acc 0.95, loss: 0.17666630537450215\n",
      "3970 train acc:0.99, test acc 0.95, loss: 0.17634276034915616\n",
      "3980 train acc:0.99, test acc 0.95, loss: 0.1760209216052141\n",
      "3990 train acc:0.99, test acc 0.95, loss: 0.17570077425512107\n",
      "4000 train acc:0.99, test acc 0.95, loss: 0.17538230358835005\n",
      "4010 train acc:0.99, test acc 0.95, loss: 0.17506549506875466\n",
      "4020 train acc:0.99, test acc 0.95, loss: 0.17475033433196907\n",
      "4030 train acc:0.99, test acc 0.95, loss: 0.17443680718285523\n",
      "4040 train acc:0.99, test acc 0.95, loss: 0.1741248995929955\n",
      "4050 train acc:0.99, test acc 0.95, loss: 0.17381459769823077\n",
      "4060 train acc:0.99, test acc 0.95, loss: 0.17350588779624065\n",
      "4070 train acc:0.99, test acc 0.95, loss: 0.17319875634416962\n",
      "4080 train acc:0.99, test acc 0.95, loss: 0.17289318995629224\n",
      "4090 train acc:0.99, test acc 0.95, loss: 0.1725891754017215\n",
      "4100 train acc:0.99, test acc 0.95, loss: 0.1722866996021567\n",
      "4110 train acc:0.99, test acc 0.95, loss: 0.17198574962967078\n",
      "4120 train acc:0.99, test acc 0.95, loss: 0.1716863127045374\n",
      "4130 train acc:0.99, test acc 0.95, loss: 0.17138837619309386\n",
      "4140 train acc:0.99, test acc 0.95, loss: 0.1710919276056435\n",
      "4150 train acc:0.99, test acc 0.95, loss: 0.17079695459439287\n",
      "4160 train acc:0.99, test acc 0.95, loss: 0.17050344495142486\n",
      "4170 train acc:0.99, test acc 0.95, loss: 0.17021138660670757\n",
      "4180 train acc:0.99, test acc 0.95, loss: 0.16992076762613587\n",
      "4190 train acc:0.99, test acc 0.95, loss: 0.16963157620960811\n",
      "4200 train acc:0.99, test acc 0.95, loss: 0.16934380068913402\n",
      "4210 train acc:0.99, test acc 0.95, loss: 0.16905742952697683\n",
      "4220 train acc:0.99, test acc 0.95, loss: 0.16877245131382473\n",
      "4230 train acc:0.99, test acc 0.95, loss: 0.1684888547669946\n",
      "4240 train acc:0.99, test acc 0.95, loss: 0.1682066287286662\n",
      "4250 train acc:0.99, test acc 0.95, loss: 0.16792576216414468\n",
      "4260 train acc:0.99, test acc 0.95, loss: 0.1676462441601541\n",
      "4270 train acc:0.99, test acc 0.95, loss: 0.1673680639231576\n",
      "4280 train acc:0.99, test acc 0.95, loss: 0.16709121077770717\n",
      "4290 train acc:0.99, test acc 0.95, loss: 0.1668156741648197\n",
      "4300 train acc:0.99, test acc 0.95, loss: 0.16654144364038057\n",
      "4310 train acc:0.99, test acc 0.95, loss: 0.16626850887357353\n",
      "4320 train acc:0.99, test acc 0.95, loss: 0.16599685964533603\n",
      "4330 train acc:0.99, test acc 0.95, loss: 0.16572648584684033\n",
      "4340 train acc:0.99, test acc 0.95, loss: 0.16545737747799966\n",
      "4350 train acc:0.99, test acc 0.95, loss: 0.16518952464599823\n",
      "4360 train acc:0.99, test acc 0.95, loss: 0.16492291756384553\n",
      "4370 train acc:0.99, test acc 0.95, loss: 0.16465754654895406\n",
      "4380 train acc:0.99, test acc 0.95, loss: 0.16439340202173974\n",
      "4390 train acc:0.99, test acc 0.95, loss: 0.16413047450424606\n",
      "4400 train acc:0.99, test acc 0.95, loss: 0.1638687546187883\n",
      "4410 train acc:0.99, test acc 0.95, loss: 0.16360823308662223\n",
      "4420 train acc:0.99, test acc 0.95, loss: 0.16334890072663136\n",
      "4430 train acc:0.99, test acc 0.95, loss: 0.1630907484540369\n",
      "4440 train acc:0.99, test acc 0.95, loss: 0.16283376727912846\n",
      "4450 train acc:0.99, test acc 0.95, loss: 0.16257794830601305\n",
      "4460 train acc:0.99, test acc 0.95, loss: 0.16232328273138605\n",
      "4470 train acc:0.99, test acc 0.95, loss: 0.16206976184332003\n",
      "4480 train acc:0.99, test acc 0.95, loss: 0.1618173770200734\n",
      "4490 train acc:0.99, test acc 0.95, loss: 0.16156611972891777\n",
      "4500 train acc:0.99, test acc 0.95, loss: 0.161315981524983\n",
      "4510 train acc:0.99, test acc 0.95, loss: 0.16106695405012122\n",
      "4520 train acc:0.99, test acc 0.95, loss: 0.16081902903178766\n",
      "4530 train acc:0.99, test acc 0.95, loss: 0.16057219828193958\n",
      "4540 train acc:0.99, test acc 0.95, loss: 0.16032645369595178\n",
      "4550 train acc:0.99, test acc 0.95, loss: 0.16008178725154895\n",
      "4560 train acc:0.99, test acc 0.95, loss: 0.15983819100775468\n",
      "4570 train acc:0.99, test acc 0.95, loss: 0.15959565710385648\n",
      "4580 train acc:0.99, test acc 0.95, loss: 0.15935417775838615\n",
      "4590 train acc:0.99, test acc 0.95, loss: 0.15911374526811695\n",
      "4600 train acc:0.99, test acc 0.95, loss: 0.1588743520070744\n",
      "4610 train acc:0.99, test acc 0.95, loss: 0.15863599042556417\n",
      "4620 train acc:0.99, test acc 0.95, loss: 0.15839865304921213\n",
      "4630 train acc:0.99, test acc 0.95, loss: 0.15816233247802178\n",
      "4640 train acc:0.99, test acc 0.95, loss: 0.15792702138544315\n",
      "4650 train acc:0.99, test acc 0.95, loss: 0.15769271251745776\n",
      "4660 train acc:0.99, test acc 0.95, loss: 0.15745939869167636\n",
      "4670 train acc:0.99, test acc 0.95, loss: 0.15722707279645007\n",
      "4680 train acc:0.99, test acc 0.95, loss: 0.15699572778999532\n",
      "4690 train acc:0.99, test acc 0.95, loss: 0.15676535669953137\n",
      "4700 train acc:0.99, test acc 0.95, loss: 0.15653595262043074\n",
      "4710 train acc:0.99, test acc 0.95, loss: 0.1563075087153824\n",
      "4720 train acc:0.99, test acc 0.95, loss: 0.15608001821356712\n",
      "4730 train acc:0.99, test acc 0.95, loss: 0.1558534744098448\n",
      "4740 train acc:0.99, test acc 0.95, loss: 0.15562787066395412\n",
      "4750 train acc:0.99, test acc 0.95, loss: 0.15540320039972388\n",
      "4760 train acc:0.99, test acc 0.95, loss: 0.15517945710429545\n",
      "4770 train acc:0.99, test acc 0.95, loss: 0.1549566343273569\n",
      "4780 train acc:0.99, test acc 0.95, loss: 0.15473472568038818\n",
      "4790 train acc:0.99, test acc 0.95, loss: 0.15451372483591735\n",
      "4800 train acc:0.99, test acc 0.95, loss: 0.15429362552678735\n",
      "4810 train acc:0.99, test acc 0.95, loss: 0.1540744215454335\n",
      "4820 train acc:0.99, test acc 0.95, loss: 0.15385610674317138\n",
      "4830 train acc:0.99, test acc 0.95, loss: 0.15363867502949496\n",
      "4840 train acc:0.99, test acc 0.95, loss: 0.1534221203713846\n",
      "4850 train acc:0.99, test acc 0.95, loss: 0.15320643679262516\n",
      "4860 train acc:0.99, test acc 0.95, loss: 0.15299161837313374\n",
      "4870 train acc:0.99, test acc 0.95, loss: 0.15277765924829712\n",
      "4880 train acc:0.99, test acc 0.95, loss: 0.15256455360831803\n",
      "4890 train acc:0.99, test acc 0.95, loss: 0.15235229569757153\n",
      "4900 train acc:0.99, test acc 0.95, loss: 0.15214087981396984\n",
      "4910 train acc:0.99, test acc 0.95, loss: 0.1519303003083362\n",
      "4920 train acc:0.99, test acc 0.95, loss: 0.1517205515837877\n",
      "4930 train acc:0.99, test acc 0.95, loss: 0.15151162809512692\n",
      "4940 train acc:0.99, test acc 0.95, loss: 0.1513035243482412\n",
      "4950 train acc:0.99, test acc 0.95, loss: 0.1510962348995117\n",
      "4960 train acc:0.99, test acc 0.95, loss: 0.1508897543552291\n",
      "4970 train acc:0.99, test acc 0.95, loss: 0.15068407737101883\n",
      "4980 train acc:0.99, test acc 0.95, loss: 0.1504791986512731\n",
      "4990 train acc:0.99, test acc 0.95, loss: 0.15027511294859178\n",
      "5000 train acc:0.99, test acc 0.95, loss: 0.15007181506323003\n",
      "5010 train acc:0.99, test acc 0.95, loss: 0.14986929984255418\n",
      "5020 train acc:0.99, test acc 0.95, loss: 0.14966756218050473\n",
      "5030 train acc:0.99, test acc 0.95, loss: 0.14946659701706735\n",
      "5040 train acc:0.99, test acc 0.95, loss: 0.14926639933774966\n",
      "5050 train acc:0.99, test acc 0.95, loss: 0.14906696417306686\n",
      "5060 train acc:0.99, test acc 0.95, loss: 0.14886828659803295\n",
      "5070 train acc:0.99, test acc 0.95, loss: 0.14867036173166\n",
      "5080 train acc:0.99, test acc 0.95, loss: 0.14847318473646287\n",
      "5090 train acc:0.99, test acc 0.95, loss: 0.14827675081797229\n",
      "5100 train acc:0.99, test acc 0.95, loss: 0.14808105522425247\n",
      "5110 train acc:0.99, test acc 0.95, loss: 0.1478860932454272\n",
      "5120 train acc:0.99, test acc 0.95, loss: 0.14769186021321087\n",
      "5130 train acc:0.99, test acc 0.95, loss: 0.14749835150044616\n",
      "5140 train acc:0.99, test acc 0.95, loss: 0.14730556252064844\n",
      "5150 train acc:0.99, test acc 0.95, loss: 0.14711348872755534\n",
      "5160 train acc:0.99, test acc 0.95, loss: 0.14692212561468287\n",
      "5170 train acc:0.99, test acc 0.95, loss: 0.14673146871488751\n",
      "5180 train acc:0.99, test acc 0.95, loss: 0.14654151359993317\n",
      "5190 train acc:0.99, test acc 0.95, loss: 0.14635225588006545\n",
      "5200 train acc:0.99, test acc 0.95, loss: 0.14616369120358982\n",
      "5210 train acc:0.99, test acc 0.95, loss: 0.14597581525645656\n",
      "5220 train acc:0.99, test acc 0.95, loss: 0.14578862376185037\n",
      "5230 train acc:0.99, test acc 0.95, loss: 0.14560211247978597\n",
      "5240 train acc:0.99, test acc 0.95, loss: 0.14541627720670816\n",
      "5250 train acc:0.99, test acc 0.95, loss: 0.14523111377509818\n",
      "5260 train acc:0.99, test acc 0.95, loss: 0.14504661805308403\n",
      "5270 train acc:0.99, test acc 0.95, loss: 0.1448627859440566\n",
      "5280 train acc:0.99, test acc 0.95, loss: 0.1446796133862906\n",
      "5290 train acc:0.99, test acc 0.95, loss: 0.14449709635257027\n",
      "5300 train acc:0.99, test acc 0.95, loss: 0.14431523084981948\n",
      "5310 train acc:0.99, test acc 0.95, loss: 0.14413401291873792\n",
      "5320 train acc:0.99, test acc 0.95, loss: 0.14395343863344037\n",
      "5330 train acc:0.99, test acc 0.95, loss: 0.14377350410110148\n",
      "5340 train acc:0.99, test acc 0.95, loss: 0.14359420546160487\n",
      "5350 train acc:0.99, test acc 0.95, loss: 0.14341553888719663\n",
      "5360 train acc:0.99, test acc 0.95, loss: 0.14323750058214363\n",
      "5370 train acc:0.99, test acc 0.95, loss: 0.14306008678239515\n",
      "5380 train acc:0.99, test acc 0.95, loss: 0.14288329375525027\n",
      "5390 train acc:0.99, test acc 0.95, loss: 0.14270711779902787\n",
      "5400 train acc:0.99, test acc 0.95, loss: 0.14253155524274214\n",
      "5410 train acc:0.99, test acc 0.95, loss: 0.1423566024457811\n",
      "5420 train acc:0.99, test acc 0.95, loss: 0.1421822557975902\n",
      "5430 train acc:0.99, test acc 0.95, loss: 0.1420085117173588\n",
      "5440 train acc:0.99, test acc 0.95, loss: 0.14183536665371113\n",
      "5450 train acc:0.99, test acc 0.95, loss: 0.1416628170844013\n",
      "5460 train acc:0.99, test acc 0.95, loss: 0.14149085951601176\n",
      "5470 train acc:0.99, test acc 0.95, loss: 0.1413194904836555\n",
      "5480 train acc:0.99, test acc 0.95, loss: 0.14114870655068182\n",
      "5490 train acc:0.99, test acc 0.95, loss: 0.14097850430838643\n",
      "5500 train acc:0.99, test acc 0.95, loss: 0.14080888037572425\n",
      "5510 train acc:0.99, test acc 0.95, loss: 0.14063983139902608\n",
      "5520 train acc:0.99, test acc 0.95, loss: 0.1404713540517191\n",
      "5530 train acc:0.99, test acc 0.95, loss: 0.14030344503405018\n",
      "5540 train acc:0.99, test acc 0.95, loss: 0.14013610107281307\n",
      "5550 train acc:0.99, test acc 0.95, loss: 0.13996931892107853\n",
      "5560 train acc:0.99, test acc 0.95, loss: 0.13980309535792818\n",
      "5570 train acc:0.99, test acc 0.95, loss: 0.13963742718819103\n",
      "5580 train acc:0.99, test acc 0.95, loss: 0.1394723112421838\n",
      "5590 train acc:0.99, test acc 0.95, loss: 0.13930774437545407\n",
      "5600 train acc:0.99, test acc 0.95, loss: 0.13914372346852627\n",
      "5610 train acc:0.99, test acc 0.95, loss: 0.13898024542665127\n",
      "5620 train acc:0.99, test acc 0.95, loss: 0.13881730717955887\n",
      "5630 train acc:0.99, test acc 0.95, loss: 0.13865490568121264\n",
      "5640 train acc:0.99, test acc 0.95, loss: 0.13849303790956877\n",
      "5650 train acc:0.99, test acc 0.95, loss: 0.13833170086633673\n",
      "5660 train acc:0.99, test acc 0.95, loss: 0.13817089157674342\n",
      "5670 train acc:0.99, test acc 0.95, loss: 0.13801060708930016\n",
      "5680 train acc:0.99, test acc 0.95, loss: 0.1378508444755721\n",
      "5690 train acc:0.99, test acc 0.95, loss: 0.1376916008299505\n",
      "5700 train acc:0.99, test acc 0.95, loss: 0.1375328732694278\n",
      "5710 train acc:0.99, test acc 0.95, loss: 0.13737465893337528\n",
      "5720 train acc:0.99, test acc 0.95, loss: 0.13721695498332334\n",
      "5730 train acc:0.99, test acc 0.95, loss: 0.13705975860274439\n",
      "5740 train acc:0.99, test acc 0.95, loss: 0.1369030669968383\n",
      "5750 train acc:0.99, test acc 0.95, loss: 0.13674687739232014\n",
      "5760 train acc:0.99, test acc 0.95, loss: 0.136591187037211\n",
      "5770 train acc:0.99, test acc 0.95, loss: 0.1364359932006302\n",
      "5780 train acc:0.99, test acc 0.95, loss: 0.13628129317259155\n",
      "5790 train acc:0.99, test acc 0.95, loss: 0.13612708426380002\n",
      "5800 train acc:0.99, test acc 0.95, loss: 0.13597336380545236\n",
      "5810 train acc:0.99, test acc 0.95, loss: 0.13582012914903907\n",
      "5820 train acc:0.99, test acc 0.95, loss: 0.1356673776661493\n",
      "5830 train acc:0.99, test acc 0.95, loss: 0.13551510674827777\n",
      "5840 train acc:0.99, test acc 0.95, loss: 0.13536331380663325\n",
      "5850 train acc:0.99, test acc 0.95, loss: 0.1352119962719507\n",
      "5860 train acc:0.99, test acc 0.95, loss: 0.13506115159430424\n",
      "5870 train acc:0.99, test acc 0.95, loss: 0.13491077724292277\n",
      "5880 train acc:0.99, test acc 0.95, loss: 0.1347608707060076\n",
      "5890 train acc:0.99, test acc 0.95, loss: 0.13461142949055258\n",
      "5900 train acc:0.99, test acc 0.95, loss: 0.13446245112216557\n",
      "5910 train acc:0.99, test acc 0.95, loss: 0.13431393314489243\n",
      "5920 train acc:0.99, test acc 0.95, loss: 0.134165873121043\n",
      "5930 train acc:0.99, test acc 0.95, loss: 0.13401826863101882\n",
      "5940 train acc:0.99, test acc 0.95, loss: 0.1338711172731426\n",
      "5950 train acc:0.99, test acc 0.95, loss: 0.1337244166634906\n",
      "5960 train acc:0.99, test acc 0.95, loss: 0.13357816443572565\n",
      "5970 train acc:0.99, test acc 0.95, loss: 0.13343235824093277\n",
      "5980 train acc:0.99, test acc 0.95, loss: 0.13328699574745662\n",
      "5990 train acc:0.99, test acc 0.95, loss: 0.13314207464074046\n",
      "6000 train acc:0.99, test acc 0.95, loss: 0.13299759262316724\n",
      "6010 train acc:0.99, test acc 0.95, loss: 0.132853547413902\n",
      "6020 train acc:0.99, test acc 0.95, loss: 0.13270993674873693\n",
      "6030 train acc:0.99, test acc 0.95, loss: 0.13256675837993676\n",
      "6040 train acc:0.99, test acc 0.95, loss: 0.13242401007608712\n",
      "6050 train acc:0.99, test acc 0.95, loss: 0.13228168962194436\n",
      "6060 train acc:0.99, test acc 0.95, loss: 0.13213979481828628\n",
      "6070 train acc:0.99, test acc 0.95, loss: 0.13199832348176518\n",
      "6080 train acc:0.99, test acc 0.95, loss: 0.13185727344476258\n",
      "6090 train acc:0.99, test acc 0.95, loss: 0.13171664255524507\n",
      "6100 train acc:0.99, test acc 0.95, loss: 0.13157642867662203\n",
      "6110 train acc:0.99, test acc 0.95, loss: 0.13143662968760497\n",
      "6120 train acc:0.99, test acc 0.95, loss: 0.1312972434820684\n",
      "6130 train acc:0.99, test acc 0.95, loss: 0.1311582679689119\n",
      "6140 train acc:0.99, test acc 0.95, loss: 0.13101970107192404\n",
      "6150 train acc:0.99, test acc 0.95, loss: 0.13088154072964764\n",
      "6160 train acc:0.99, test acc 0.95, loss: 0.13074378489524666\n",
      "6170 train acc:0.99, test acc 0.95, loss: 0.13060643153637413\n",
      "6180 train acc:0.99, test acc 0.95, loss: 0.1304694786350422\n",
      "6190 train acc:0.99, test acc 0.95, loss: 0.13033292418749234\n",
      "6200 train acc:0.99, test acc 0.95, loss: 0.130196766204069\n",
      "6210 train acc:0.99, test acc 0.95, loss: 0.13006100270909232\n",
      "6220 train acc:0.99, test acc 0.95, loss: 0.12992563174073352\n",
      "6230 train acc:0.99, test acc 0.95, loss: 0.12979065135089202\n",
      "6240 train acc:0.99, test acc 0.95, loss: 0.12965605960507243\n",
      "6250 train acc:0.99, test acc 0.95, loss: 0.1295218545822641\n",
      "6260 train acc:0.99, test acc 0.95, loss: 0.12938803437482155\n",
      "6270 train acc:0.99, test acc 0.95, loss: 0.12925459708834575\n",
      "6280 train acc:0.99, test acc 0.95, loss: 0.1291215408415678\n",
      "6290 train acc:0.99, test acc 0.95, loss: 0.12898886376623223\n",
      "6300 train acc:0.99, test acc 0.95, loss: 0.12885656400698328\n",
      "6310 train acc:0.99, test acc 0.95, loss: 0.1287246397212506\n",
      "6320 train acc:0.99, test acc 0.95, loss: 0.12859308907913797\n",
      "6330 train acc:0.99, test acc 0.95, loss: 0.12846191026331177\n",
      "6340 train acc:0.99, test acc 0.95, loss: 0.12833110146889112\n",
      "6350 train acc:0.99, test acc 0.95, loss: 0.1282006609033396\n",
      "6360 train acc:0.99, test acc 0.95, loss: 0.12807058678635716\n",
      "6370 train acc:0.99, test acc 0.95, loss: 0.12794087734977433\n",
      "6380 train acc:0.99, test acc 0.95, loss: 0.12781153083744656\n",
      "6390 train acc:0.99, test acc 0.95, loss: 0.12768254550514987\n",
      "6400 train acc:0.99, test acc 0.95, loss: 0.127553919620478\n",
      "6410 train acc:0.99, test acc 0.95, loss: 0.12742565146274049\n",
      "6420 train acc:0.99, test acc 0.95, loss: 0.12729773932286123\n",
      "6430 train acc:0.99, test acc 0.95, loss: 0.12717018150327894\n",
      "6440 train acc:0.99, test acc 0.95, loss: 0.12704297631784833\n",
      "6450 train acc:0.99, test acc 0.95, loss: 0.12691612209174158\n",
      "6460 train acc:0.99, test acc 0.95, loss: 0.12678961716135217\n",
      "6470 train acc:0.99, test acc 0.95, loss: 0.12666345987419875\n",
      "6480 train acc:0.99, test acc 0.95, loss: 0.12653764858883013\n",
      "6490 train acc:0.99, test acc 0.95, loss: 0.12641218167473162\n",
      "6500 train acc:0.99, test acc 0.95, loss: 0.1262870575122317\n",
      "6510 train acc:0.99, test acc 0.95, loss: 0.12616227449241046\n",
      "6520 train acc:0.99, test acc 0.95, loss: 0.1260378310170083\n",
      "6530 train acc:0.99, test acc 0.95, loss: 0.12591372549833557\n",
      "6540 train acc:0.99, test acc 0.95, loss: 0.1257899563591839\n",
      "6550 train acc:0.99, test acc 0.95, loss: 0.1256665220327373\n",
      "6560 train acc:0.99, test acc 0.95, loss: 0.12554342096248522\n",
      "6570 train acc:0.99, test acc 0.95, loss: 0.12542065160213578\n",
      "6580 train acc:0.99, test acc 0.95, loss: 0.12529821241553005\n",
      "6590 train acc:0.99, test acc 0.95, loss: 0.12517610187655734\n",
      "6600 train acc:0.99, test acc 0.95, loss: 0.12505431846907145\n",
      "6610 train acc:0.99, test acc 0.95, loss: 0.12493286068680703\n",
      "6620 train acc:0.99, test acc 0.95, loss: 0.12481172703329807\n",
      "6630 train acc:0.99, test acc 0.95, loss: 0.12469091602179598\n",
      "6640 train acc:0.99, test acc 0.95, loss: 0.12457042617518882\n",
      "6650 train acc:0.99, test acc 0.95, loss: 0.12445025602592187\n",
      "6660 train acc:0.99, test acc 0.95, loss: 0.12433040411591849\n",
      "6670 train acc:0.99, test acc 0.95, loss: 0.12421086899650176\n",
      "6680 train acc:0.99, test acc 0.95, loss: 0.12409164922831727\n",
      "6690 train acc:0.99, test acc 0.95, loss: 0.12397274338125613\n",
      "6700 train acc:0.99, test acc 0.95, loss: 0.12385415003437922\n",
      "6710 train acc:0.99, test acc 0.95, loss: 0.1237358677758419\n",
      "6720 train acc:0.99, test acc 0.95, loss: 0.12361789520281986\n",
      "6730 train acc:0.99, test acc 0.95, loss: 0.12350023092143492\n",
      "6740 train acc:0.99, test acc 0.95, loss: 0.12338287354668263\n",
      "6750 train acc:0.99, test acc 0.95, loss: 0.12326582170235958\n",
      "6760 train acc:0.99, test acc 0.95, loss: 0.12314907402099211\n",
      "6770 train acc:0.99, test acc 0.95, loss: 0.12303262914376514\n",
      "6780 train acc:0.99, test acc 0.95, loss: 0.1229164857204525\n",
      "6790 train acc:0.99, test acc 0.95, loss: 0.12280064240934696\n",
      "6800 train acc:0.99, test acc 0.95, loss: 0.12268509787719167\n",
      "6810 train acc:0.99, test acc 0.95, loss: 0.12256985079911242\n",
      "6820 train acc:0.99, test acc 0.95, loss: 0.12245489985854915\n",
      "6830 train acc:0.99, test acc 0.95, loss: 0.12234024374719041\n",
      "6840 train acc:0.99, test acc 0.95, loss: 0.1222258811649062\n",
      "6850 train acc:0.99, test acc 0.95, loss: 0.12211181081968299\n",
      "6860 train acc:0.99, test acc 0.95, loss: 0.12199803142755858\n",
      "6870 train acc:0.99, test acc 0.95, loss: 0.12188454171255798\n",
      "6880 train acc:0.99, test acc 0.95, loss: 0.12177134040662979\n",
      "6890 train acc:0.99, test acc 0.95, loss: 0.12165842624958303\n",
      "6900 train acc:0.99, test acc 0.95, loss: 0.12154579798902486\n",
      "6910 train acc:0.99, test acc 0.95, loss: 0.12143345438029857\n",
      "6920 train acc:0.99, test acc 0.95, loss: 0.12132139418642271\n",
      "6930 train acc:0.99, test acc 0.95, loss: 0.12120961617802972\n",
      "6940 train acc:0.99, test acc 0.95, loss: 0.12109811913330694\n",
      "6950 train acc:0.99, test acc 0.95, loss: 0.12098690183793598\n",
      "6960 train acc:0.99, test acc 0.95, loss: 0.12087596308503473\n",
      "6970 train acc:0.99, test acc 0.95, loss: 0.12076530167509812\n",
      "6980 train acc:0.99, test acc 0.95, loss: 0.12065491641594127\n",
      "6990 train acc:0.99, test acc 0.95, loss: 0.12054480612264112\n",
      "7000 train acc:0.99, test acc 0.95, loss: 0.12043496961748074\n",
      "7010 train acc:0.99, test acc 0.95, loss: 0.12032540572989245\n",
      "7020 train acc:0.99, test acc 0.95, loss: 0.12021611329640228\n",
      "7030 train acc:0.99, test acc 0.95, loss: 0.12010709116057514\n",
      "7040 train acc:0.99, test acc 0.95, loss: 0.11999833817295981\n",
      "7050 train acc:0.99, test acc 0.95, loss: 0.11988985319103496\n",
      "7060 train acc:0.99, test acc 0.95, loss: 0.11978163507915575\n",
      "7070 train acc:0.99, test acc 0.95, loss: 0.11967368270850011\n",
      "7080 train acc:0.99, test acc 0.95, loss: 0.11956599495701713\n",
      "7090 train acc:0.99, test acc 0.95, loss: 0.11945857070937395\n",
      "7100 train acc:0.99, test acc 0.95, loss: 0.1193514088569047\n",
      "7110 train acc:0.99, test acc 0.95, loss: 0.11924450829755891\n",
      "7120 train acc:0.99, test acc 0.95, loss: 0.11913786793585157\n",
      "7130 train acc:0.99, test acc 0.95, loss: 0.11903148668281205\n",
      "7140 train acc:0.99, test acc 0.95, loss: 0.11892536345593493\n",
      "7150 train acc:0.99, test acc 0.95, loss: 0.11881949717913053\n",
      "7160 train acc:0.99, test acc 0.95, loss: 0.11871388678267591\n",
      "7170 train acc:0.99, test acc 0.95, loss: 0.11860853120316683\n",
      "7180 train acc:0.99, test acc 0.95, loss: 0.11850342938346971\n",
      "7190 train acc:0.99, test acc 0.95, loss: 0.11839858027267358\n",
      "7200 train acc:0.99, test acc 0.95, loss: 0.11829398282604404\n",
      "7210 train acc:0.99, test acc 0.95, loss: 0.11818963600497578\n",
      "7220 train acc:0.99, test acc 0.95, loss: 0.11808553877694676\n",
      "7230 train acc:0.99, test acc 0.95, loss: 0.11798169011547213\n",
      "7240 train acc:0.99, test acc 0.95, loss: 0.11787808900005929\n",
      "7250 train acc:0.99, test acc 0.95, loss: 0.11777473441616264\n",
      "7260 train acc:0.99, test acc 0.95, loss: 0.1176716253551389\n",
      "7270 train acc:0.99, test acc 0.95, loss: 0.1175687608142036\n",
      "7280 train acc:0.99, test acc 0.95, loss: 0.11746613979638623\n",
      "7290 train acc:0.99, test acc 0.95, loss: 0.11736376131048813\n",
      "7300 train acc:0.99, test acc 0.95, loss: 0.11726162437103867\n",
      "7310 train acc:0.99, test acc 0.95, loss: 0.11715972799825305\n",
      "7320 train acc:0.99, test acc 0.95, loss: 0.11705807121798979\n",
      "7330 train acc:0.99, test acc 0.95, loss: 0.11695665306170965\n",
      "7340 train acc:0.99, test acc 0.95, loss: 0.11685547256643311\n",
      "7350 train acc:0.99, test acc 0.95, loss: 0.11675452877470026\n",
      "7360 train acc:0.99, test acc 0.95, loss: 0.11665382073452969\n",
      "7370 train acc:0.99, test acc 0.95, loss: 0.11655334749937798\n",
      "7380 train acc:0.99, test acc 0.95, loss: 0.11645310812810022\n",
      "7390 train acc:0.99, test acc 0.95, loss: 0.11635310168491006\n",
      "7400 train acc:0.99, test acc 0.95, loss: 0.11625332723934034\n",
      "7410 train acc:0.99, test acc 0.95, loss: 0.11615378386620465\n",
      "7420 train acc:0.99, test acc 0.95, loss: 0.11605447064555818\n",
      "7430 train acc:0.99, test acc 0.95, loss: 0.11595538666265977\n",
      "7440 train acc:0.99, test acc 0.95, loss: 0.11585653100793426\n",
      "7450 train acc:0.99, test acc 0.95, loss: 0.11575790277693448\n",
      "7460 train acc:0.99, test acc 0.95, loss: 0.1156595010703042\n",
      "7470 train acc:0.99, test acc 0.95, loss: 0.11556132499374151\n",
      "7480 train acc:0.99, test acc 0.95, loss: 0.1154633736579617\n",
      "7490 train acc:0.99, test acc 0.95, loss: 0.11536564617866142\n",
      "7500 train acc:0.99, test acc 0.95, loss: 0.11526814167648271\n",
      "7510 train acc:0.99, test acc 0.95, loss: 0.1151708592769772\n",
      "7520 train acc:0.99, test acc 0.95, loss: 0.11507379811057108\n",
      "7530 train acc:0.99, test acc 0.95, loss: 0.1149769573125296\n",
      "7540 train acc:0.99, test acc 0.95, loss: 0.11488033602292301\n",
      "7550 train acc:0.99, test acc 0.95, loss: 0.11478393338659161\n",
      "7560 train acc:0.99, test acc 0.95, loss: 0.11468774855311206\n",
      "7570 train acc:0.99, test acc 0.95, loss: 0.11459178067676332\n",
      "7580 train acc:0.99, test acc 0.95, loss: 0.11449602891649327\n",
      "7590 train acc:0.99, test acc 0.95, loss: 0.1144004924358855\n",
      "7600 train acc:0.99, test acc 0.95, loss: 0.11430517040312627\n",
      "7610 train acc:0.99, test acc 0.95, loss: 0.11421006199097192\n",
      "7620 train acc:0.99, test acc 0.95, loss: 0.11411516637671645\n",
      "7630 train acc:0.99, test acc 0.95, loss: 0.11402048274215987\n",
      "7640 train acc:0.99, test acc 0.95, loss: 0.1139260102735755\n",
      "7650 train acc:0.99, test acc 0.95, loss: 0.11383174816167924\n",
      "7660 train acc:0.99, test acc 0.95, loss: 0.11373769560159794\n",
      "7670 train acc:0.99, test acc 0.95, loss: 0.11364385179283853\n",
      "7680 train acc:0.99, test acc 0.95, loss: 0.11355021593925707\n",
      "7690 train acc:0.99, test acc 0.95, loss: 0.11345678724902868\n",
      "7700 train acc:0.99, test acc 0.95, loss: 0.11336356493461673\n",
      "7710 train acc:0.99, test acc 0.95, loss: 0.11327054821274349\n",
      "7720 train acc:0.99, test acc 0.95, loss: 0.11317773630435993\n",
      "7730 train acc:0.99, test acc 0.95, loss: 0.11308512843461677\n",
      "7740 train acc:0.99, test acc 0.95, loss: 0.1129927238328348\n",
      "7750 train acc:0.99, test acc 0.95, loss: 0.11290052173247604\n",
      "7760 train acc:0.99, test acc 0.95, loss: 0.11280852137111537\n",
      "7770 train acc:0.99, test acc 0.95, loss: 0.11271672199041168\n",
      "7780 train acc:0.99, test acc 0.95, loss: 0.11262512283607952\n",
      "7790 train acc:0.99, test acc 0.95, loss: 0.11253372315786184\n",
      "7800 train acc:0.99, test acc 0.95, loss: 0.11244252220950124\n",
      "7810 train acc:0.99, test acc 0.95, loss: 0.11235151924871323\n",
      "7820 train acc:0.99, test acc 0.95, loss: 0.11226071353715854\n",
      "7830 train acc:0.99, test acc 0.95, loss: 0.11217010434041617\n",
      "7840 train acc:0.99, test acc 0.95, loss: 0.1120796909279566\n",
      "7850 train acc:0.99, test acc 0.95, loss: 0.11198947257311494\n",
      "7860 train acc:0.99, test acc 0.95, loss: 0.11189944855306493\n",
      "7870 train acc:0.99, test acc 0.95, loss: 0.11180961814879241\n",
      "7880 train acc:0.99, test acc 0.95, loss: 0.1117199806450693\n",
      "7890 train acc:0.99, test acc 0.95, loss: 0.11163053533042831\n",
      "7900 train acc:0.99, test acc 0.95, loss: 0.11154128149713713\n",
      "7910 train acc:0.99, test acc 0.95, loss: 0.1114522184411728\n",
      "7920 train acc:0.99, test acc 0.95, loss: 0.1113633454621974\n",
      "7930 train acc:0.99, test acc 0.95, loss: 0.11127466186353219\n",
      "7940 train acc:0.99, test acc 0.95, loss: 0.1111861669521336\n",
      "7950 train acc:0.99, test acc 0.95, loss: 0.11109786003856843\n",
      "7960 train acc:0.99, test acc 0.95, loss: 0.1110097404369898\n",
      "7970 train acc:0.99, test acc 0.95, loss: 0.11092180746511265\n",
      "7980 train acc:0.99, test acc 0.95, loss: 0.11083406044419036\n",
      "7990 train acc:0.99, test acc 0.95, loss: 0.11074649869899027\n",
      "8000 train acc:0.99, test acc 0.95, loss: 0.11065912155777116\n",
      "8010 train acc:0.99, test acc 0.95, loss: 0.11057192835225921\n",
      "8020 train acc:0.99, test acc 0.95, loss: 0.11048491841762503\n",
      "8030 train acc:0.99, test acc 0.95, loss: 0.110398091092461\n",
      "8040 train acc:0.99, test acc 0.95, loss: 0.11031144571875806\n",
      "8050 train acc:0.99, test acc 0.95, loss: 0.11022498164188338\n",
      "8060 train acc:0.99, test acc 0.95, loss: 0.11013869821055802\n",
      "8070 train acc:0.99, test acc 0.95, loss: 0.1100525947768348\n",
      "8080 train acc:0.99, test acc 0.95, loss: 0.10996667069607576\n",
      "8090 train acc:0.99, test acc 0.95, loss: 0.10988092532693093\n",
      "8100 train acc:0.99, test acc 0.95, loss: 0.10979535803131667\n",
      "8110 train acc:0.99, test acc 0.95, loss: 0.1097099681743932\n",
      "8120 train acc:0.99, test acc 0.95, loss: 0.10962475512454448\n",
      "8130 train acc:0.99, test acc 0.95, loss: 0.10953971825335666\n",
      "8140 train acc:0.99, test acc 0.95, loss: 0.10945485693559645\n",
      "8150 train acc:0.99, test acc 0.95, loss: 0.10937017054919121\n",
      "8160 train acc:0.99, test acc 0.95, loss: 0.10928565847520774\n",
      "8170 train acc:0.99, test acc 0.95, loss: 0.10920132009783216\n",
      "8180 train acc:0.99, test acc 0.95, loss: 0.10911715480434908\n",
      "8190 train acc:0.99, test acc 0.95, loss: 0.10903316198512203\n",
      "8200 train acc:0.99, test acc 0.95, loss: 0.10894934103357273\n",
      "8210 train acc:0.99, test acc 0.95, loss: 0.10886569134616243\n",
      "8220 train acc:0.99, test acc 0.95, loss: 0.1087822123223705\n",
      "8230 train acc:0.99, test acc 0.95, loss: 0.10869890336467661\n",
      "8240 train acc:0.99, test acc 0.95, loss: 0.10861576387854015\n",
      "8250 train acc:0.99, test acc 0.95, loss: 0.10853279327238168\n",
      "8260 train acc:0.99, test acc 0.95, loss: 0.10844999095756362\n",
      "8270 train acc:0.99, test acc 0.95, loss: 0.10836735634837112\n",
      "8280 train acc:0.99, test acc 0.95, loss: 0.10828488886199368\n",
      "8290 train acc:0.99, test acc 0.95, loss: 0.10820258791850622\n",
      "8300 train acc:0.99, test acc 0.95, loss: 0.10812045294085072\n",
      "8310 train acc:0.99, test acc 0.95, loss: 0.10803848335481796\n",
      "8320 train acc:0.99, test acc 0.95, loss: 0.10795667858902902\n",
      "8330 train acc:0.99, test acc 0.95, loss: 0.10787503807491756\n",
      "8340 train acc:0.99, test acc 0.95, loss: 0.10779356124671176\n",
      "8350 train acc:0.99, test acc 0.95, loss: 0.10771224754141662\n",
      "8360 train acc:0.99, test acc 0.95, loss: 0.10763109639879608\n",
      "8370 train acc:0.99, test acc 0.95, loss: 0.10755010726135579\n",
      "8380 train acc:0.99, test acc 0.95, loss: 0.10746927957432571\n",
      "8390 train acc:0.99, test acc 0.95, loss: 0.10738861278564282\n",
      "8400 train acc:0.99, test acc 0.95, loss: 0.1073081063459339\n",
      "8410 train acc:0.99, test acc 0.95, loss: 0.10722775970849874\n",
      "8420 train acc:0.99, test acc 0.95, loss: 0.10714757232929337\n",
      "8430 train acc:0.99, test acc 0.95, loss: 0.10706754366691314\n",
      "8440 train acc:0.99, test acc 0.95, loss: 0.10698767318257624\n",
      "8450 train acc:0.99, test acc 0.95, loss: 0.10690796034010719\n",
      "8460 train acc:0.99, test acc 0.95, loss: 0.10682840460592073\n",
      "8470 train acc:0.99, test acc 0.95, loss: 0.10674900544900538\n",
      "8480 train acc:0.99, test acc 0.95, loss: 0.10666976234090737\n",
      "8490 train acc:0.99, test acc 0.95, loss: 0.10659067475571486\n",
      "8500 train acc:0.99, test acc 0.95, loss: 0.10651174217004215\n",
      "8510 train acc:0.99, test acc 0.95, loss: 0.10643296406301364\n",
      "8520 train acc:0.99, test acc 0.95, loss: 0.10635433991624837\n",
      "8530 train acc:0.99, test acc 0.95, loss: 0.1062758692138449\n",
      "8540 train acc:0.99, test acc 0.95, loss: 0.10619755144236542\n",
      "8550 train acc:0.99, test acc 0.95, loss: 0.1061193860908209\n",
      "8560 train acc:0.99, test acc 0.95, loss: 0.10604137265065573\n",
      "8570 train acc:0.99, test acc 0.95, loss: 0.10596351061573282\n",
      "8580 train acc:0.99, test acc 0.95, loss: 0.10588579948231878\n",
      "8590 train acc:0.99, test acc 0.95, loss: 0.10580823874906888\n",
      "8600 train acc:0.99, test acc 0.95, loss: 0.10573082791701265\n",
      "8610 train acc:0.99, test acc 0.95, loss: 0.10565356648953929\n",
      "8620 train acc:0.99, test acc 0.95, loss: 0.10557645397238295\n",
      "8630 train acc:0.99, test acc 0.95, loss: 0.10549948987360862\n",
      "8640 train acc:0.99, test acc 0.95, loss: 0.1054226737035979\n",
      "8650 train acc:0.99, test acc 0.95, loss: 0.10534600497503468\n",
      "8660 train acc:0.99, test acc 0.95, loss: 0.10526948320289144\n",
      "8670 train acc:0.99, test acc 0.95, loss: 0.10519310790441498\n",
      "8680 train acc:0.99, test acc 0.95, loss: 0.10511687859911273\n",
      "8690 train acc:0.99, test acc 0.95, loss: 0.10504079480873914\n",
      "8700 train acc:0.99, test acc 0.95, loss: 0.10496485605728195\n",
      "8710 train acc:0.99, test acc 0.95, loss: 0.10488906187094858\n",
      "8720 train acc:0.99, test acc 0.95, loss: 0.10481341177815308\n",
      "8730 train acc:0.99, test acc 0.95, loss: 0.10473790530950242\n",
      "8740 train acc:0.99, test acc 0.95, loss: 0.10466254199778341\n",
      "8750 train acc:0.99, test acc 0.95, loss: 0.10458732137794974\n",
      "8760 train acc:0.99, test acc 0.95, loss: 0.10451224298710862\n",
      "8770 train acc:0.99, test acc 0.95, loss: 0.10443730636450825\n",
      "8780 train acc:0.99, test acc 0.95, loss: 0.10436251105152466\n",
      "8790 train acc:0.99, test acc 0.95, loss: 0.10428785659164932\n",
      "8800 train acc:0.99, test acc 0.95, loss: 0.10421334253047589\n",
      "8810 train acc:0.99, test acc 0.95, loss: 0.10413896841568845\n",
      "8820 train acc:0.99, test acc 0.95, loss: 0.10406473379704848\n",
      "8830 train acc:0.99, test acc 0.95, loss: 0.10399063822638262\n",
      "8840 train acc:0.99, test acc 0.95, loss: 0.10391668125757066\n",
      "8850 train acc:0.99, test acc 0.95, loss: 0.10384286244653282\n",
      "8860 train acc:0.99, test acc 0.95, loss: 0.10376918135121813\n",
      "8870 train acc:0.99, test acc 0.95, loss: 0.10369563753159233\n",
      "8880 train acc:0.99, test acc 0.95, loss: 0.10362223054962559\n",
      "8890 train acc:0.99, test acc 0.95, loss: 0.10354895996928123\n",
      "8900 train acc:0.99, test acc 0.95, loss: 0.10347582535650351\n",
      "8910 train acc:0.99, test acc 0.95, loss: 0.10340282627920627\n",
      "8920 train acc:0.99, test acc 0.95, loss: 0.10332996230726095\n",
      "8930 train acc:0.99, test acc 0.95, loss: 0.10325723301248547\n",
      "8940 train acc:0.99, test acc 0.95, loss: 0.10318463796863296\n",
      "8950 train acc:0.99, test acc 0.95, loss: 0.10311217675137956\n",
      "8960 train acc:0.99, test acc 0.95, loss: 0.1030398489383144\n",
      "8970 train acc:0.99, test acc 0.95, loss: 0.10296765410892714\n",
      "8980 train acc:0.99, test acc 0.95, loss: 0.10289559184459823\n",
      "8990 train acc:0.99, test acc 0.95, loss: 0.10282366172858654\n",
      "9000 train acc:0.99, test acc 0.95, loss: 0.10275186334601927\n",
      "9010 train acc:0.99, test acc 0.95, loss: 0.10268019628388107\n",
      "9020 train acc:0.99, test acc 0.95, loss: 0.10260866013100287\n",
      "9030 train acc:0.99, test acc 0.95, loss: 0.10253725447805148\n",
      "9040 train acc:0.99, test acc 0.95, loss: 0.10246597891751881\n",
      "9050 train acc:0.99, test acc 0.95, loss: 0.10239483304371129\n",
      "9060 train acc:0.99, test acc 0.95, loss: 0.10232381645273972\n",
      "9070 train acc:0.99, test acc 0.95, loss: 0.10225292874250837\n",
      "9080 train acc:0.99, test acc 0.95, loss: 0.10218216951270499\n",
      "9090 train acc:0.99, test acc 0.95, loss: 0.10211153836479035\n",
      "9100 train acc:0.99, test acc 0.95, loss: 0.10204103490198843\n",
      "9110 train acc:0.99, test acc 0.95, loss: 0.10197065872927569\n",
      "9120 train acc:0.99, test acc 0.95, loss: 0.1019004094533717\n",
      "9130 train acc:0.99, test acc 0.95, loss: 0.10183028668272859\n",
      "9140 train acc:0.99, test acc 0.95, loss: 0.10176029002752156\n",
      "9150 train acc:0.99, test acc 0.95, loss: 0.10169041909963879\n",
      "9160 train acc:0.99, test acc 0.95, loss: 0.10162067351267191\n",
      "9170 train acc:0.99, test acc 0.95, loss: 0.10155105288190597\n",
      "9180 train acc:0.99, test acc 0.95, loss: 0.10148155682431016\n",
      "9190 train acc:0.99, test acc 0.95, loss: 0.10141218495852812\n",
      "9200 train acc:0.99, test acc 0.95, loss: 0.10134293690486819\n",
      "9210 train acc:0.99, test acc 0.95, loss: 0.10127381228529451\n",
      "9220 train acc:0.99, test acc 0.95, loss: 0.10120481072341722\n",
      "9230 train acc:0.99, test acc 0.95, loss: 0.1011359318444833\n",
      "9240 train acc:0.99, test acc 0.95, loss: 0.10106717527536743\n",
      "9250 train acc:0.99, test acc 0.95, loss: 0.10099854064456258\n",
      "9260 train acc:0.99, test acc 0.95, loss: 0.10093002758217111\n",
      "9270 train acc:0.99, test acc 0.95, loss: 0.10086163571989591\n",
      "9280 train acc:0.99, test acc 0.95, loss: 0.10079336469103073\n",
      "9290 train acc:0.99, test acc 0.95, loss: 0.10072521413045209\n",
      "9300 train acc:0.99, test acc 0.95, loss: 0.1006571836746099\n",
      "9310 train acc:0.99, test acc 0.95, loss: 0.10058927296151872\n",
      "9320 train acc:0.99, test acc 0.95, loss: 0.10052148163074932\n",
      "9330 train acc:0.99, test acc 0.95, loss: 0.10045380932341957\n",
      "9340 train acc:0.99, test acc 0.95, loss: 0.10038625568218613\n",
      "9350 train acc:0.99, test acc 0.95, loss: 0.1003188203512359\n",
      "9360 train acc:0.99, test acc 0.95, loss: 0.10025150297627725\n",
      "9370 train acc:0.99, test acc 0.95, loss: 0.10018430320453182\n",
      "9380 train acc:0.99, test acc 0.95, loss: 0.10011722068472613\n",
      "9390 train acc:0.99, test acc 0.95, loss: 0.10005025506708323\n",
      "9400 train acc:0.99, test acc 0.95, loss: 0.09998340600331408\n",
      "9410 train acc:0.99, test acc 0.95, loss: 0.09991667314660996\n",
      "9420 train acc:0.99, test acc 0.95, loss: 0.09985005615163382\n",
      "9430 train acc:0.99, test acc 0.95, loss: 0.09978355467451251\n",
      "9440 train acc:0.99, test acc 0.95, loss: 0.09971716837282824\n",
      "9450 train acc:0.99, test acc 0.95, loss: 0.09965089690561127\n",
      "9460 train acc:0.99, test acc 0.95, loss: 0.09958473993333145\n",
      "9470 train acc:0.99, test acc 0.95, loss: 0.09951869711789041\n",
      "9480 train acc:0.99, test acc 0.95, loss: 0.09945276812261401\n",
      "9490 train acc:0.99, test acc 0.95, loss: 0.0993869526122441\n",
      "9500 train acc:0.99, test acc 0.95, loss: 0.09932125025293112\n",
      "9510 train acc:0.99, test acc 0.95, loss: 0.09925566071222659\n",
      "9520 train acc:0.99, test acc 0.95, loss: 0.0991901836590748\n",
      "9530 train acc:0.99, test acc 0.95, loss: 0.09912481876380586\n",
      "9540 train acc:0.99, test acc 0.95, loss: 0.09905956569812815\n",
      "9550 train acc:0.99, test acc 0.95, loss: 0.09899442413512054\n",
      "9560 train acc:0.99, test acc 0.95, loss: 0.09892939374922477\n",
      "9570 train acc:0.99, test acc 0.95, loss: 0.09886447421623908\n",
      "9580 train acc:0.99, test acc 0.95, loss: 0.09879966521330978\n",
      "9590 train acc:0.99, test acc 0.95, loss: 0.09873496641892425\n",
      "9600 train acc:0.99, test acc 0.95, loss: 0.09867037751290411\n",
      "9610 train acc:0.99, test acc 0.95, loss: 0.09860589817639777\n",
      "9620 train acc:0.99, test acc 0.95, loss: 0.09854152809187329\n",
      "9630 train acc:0.99, test acc 0.95, loss: 0.09847726694311117\n",
      "9640 train acc:0.99, test acc 0.95, loss: 0.09841311441519757\n",
      "9650 train acc:0.99, test acc 0.95, loss: 0.09834907019451722\n",
      "9660 train acc:0.99, test acc 0.95, loss: 0.09828513396874632\n",
      "9670 train acc:0.99, test acc 0.95, loss: 0.09822130542684578\n",
      "9680 train acc:0.99, test acc 0.95, loss: 0.09815758425905445\n",
      "9690 train acc:0.99, test acc 0.95, loss: 0.09809397015688198\n",
      "9700 train acc:0.99, test acc 0.95, loss: 0.09803046281310257\n",
      "9710 train acc:0.99, test acc 0.95, loss: 0.0979670619217477\n",
      "9720 train acc:0.99, test acc 0.95, loss: 0.0979037671780996\n",
      "9730 train acc:0.99, test acc 0.95, loss: 0.09784057827868513\n",
      "9740 train acc:0.99, test acc 0.95, loss: 0.09777749492126821\n",
      "9750 train acc:0.99, test acc 0.95, loss: 0.09771451680484428\n",
      "9760 train acc:0.99, test acc 0.95, loss: 0.09765164362963306\n",
      "9770 train acc:0.99, test acc 0.95, loss: 0.09758887509707248\n",
      "9780 train acc:0.99, test acc 0.95, loss: 0.09752621090981199\n",
      "9790 train acc:0.99, test acc 0.95, loss: 0.09746365077170649\n",
      "9800 train acc:0.99, test acc 0.95, loss: 0.09740119438780964\n",
      "9810 train acc:0.99, test acc 0.95, loss: 0.09733884146436779\n",
      "9820 train acc:0.99, test acc 0.95, loss: 0.09727659170881399\n",
      "9830 train acc:0.99, test acc 0.95, loss: 0.09721444482976076\n",
      "9840 train acc:0.99, test acc 0.95, loss: 0.0971524005369952\n",
      "9850 train acc:0.99, test acc 0.95, loss: 0.09709045854147194\n",
      "9860 train acc:0.99, test acc 0.95, loss: 0.09702861855530745\n",
      "9870 train acc:0.99, test acc 0.95, loss: 0.09696688029177385\n",
      "9880 train acc:0.99, test acc 0.95, loss: 0.09690524346529296\n",
      "9890 train acc:0.99, test acc 0.95, loss: 0.09684370779143002\n",
      "9900 train acc:0.99, test acc 0.95, loss: 0.09678227298688846\n",
      "9910 train acc:0.99, test acc 0.95, loss: 0.09672093876950308\n",
      "9920 train acc:0.99, test acc 0.95, loss: 0.09665970485823491\n",
      "9930 train acc:0.99, test acc 0.95, loss: 0.09659857097316497\n",
      "9940 train acc:0.99, test acc 0.95, loss: 0.09653753683548835\n",
      "9950 train acc:0.99, test acc 0.95, loss: 0.0964766021675093\n",
      "9960 train acc:0.99, test acc 0.95, loss: 0.09641576669263435\n",
      "9970 train acc:0.99, test acc 0.95, loss: 0.09635503013536746\n",
      "9980 train acc:0.99, test acc 0.95, loss: 0.0962943922213039\n",
      "9990 train acc:0.99, test acc 0.95, loss: 0.0962338526771251\n"
     ]
    }
   ],
   "source": [
    "#제 1, 2, 3, 4사분면 추정 시 정확도와 loss 측정\n",
    "import numpy as np\n",
    "import random\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): #입력 크기, 히든 레이어 크기, 출력 크기, 실수값으로 조지기\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #입력x히든 사이즈로 난수 배열 만들고 실수로 값 변환\n",
    "        self.params['b1'] = np.zeros(hidden_size)   #히든 레이어 개수만큼 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) #np.sum(y==t) y==t인 case의 횟수를 저장, 이를 평균을 내줌\n",
    "        return accuracy                               #그렇게 구한 정확도를 반환\n",
    "    \n",
    "    def numerical_gradient(self, x, t):                 #손실함수를 w에 대해 미분함 -> gradient를 반환\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):   #x=20x2, t=20x1\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        batch_num = x.shape[0]  #20\n",
    "        #forward\n",
    "        a1 = np.dot(x, W1) + b1 \n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        #backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        return grads\n",
    "\n",
    "def one_hot(label, num_classes=4):\n",
    "    one_hot = np.zeros((len(label), num_classes))\n",
    "    for i, label in enumerate(label):\n",
    "        one_hot[i, label-1] = 1\n",
    "    return one_hot\n",
    "\n",
    "#학습데이터 20개 이상 생성: 100개\n",
    "train_x = np.random.uniform(-5, 5, (100, 2))\n",
    "train_x = np.round(train_x, decimals=2)\n",
    "\n",
    "# 출력 레이블: 1, 2, 3, 4\n",
    "train_t = np.where((train_x[:, 0] > 0) & (train_x[:, 1] > 0), 1, \n",
    "    np.where((train_x[:, 0] < 0) & (train_x[:, 1] > 0), 2,\n",
    "    np.where((train_x[:, 0] < 0) & (train_x[:, 1] < 0), 3,\n",
    "    np.where((train_x[:, 0] > 0) & (train_x[:, 1] < 0), 4,  \n",
    "    0)))).reshape(-1, 1)\n",
    "train_t = one_hot(train_t)\n",
    "\n",
    "# 테스트 데이터 4개 이상: 20\n",
    "test_x = np.random.uniform(-5, 5, (20, 2))\n",
    "test_x = np.round(test_x, decimals=2)\n",
    "\n",
    "# x_test의 예상 레이블 4개\n",
    "test_t = np.where((test_x[:, 0] > 0) & (test_x[:, 1] > 0), 1, \n",
    "    np.where((test_x[:, 0] < 0) & (test_x[:, 1] > 0), 2,\n",
    "    np.where((test_x[:, 0] < 0) & (test_x[:, 1] < 0), 3,\n",
    "    np.where((test_x[:, 0] > 0) & (test_x[:, 1] < 0), 4,  \n",
    "    0)))).reshape(-1, 1)\n",
    "test_t = one_hot(test_t)\n",
    "\n",
    "network = TwoLayerNet(2, 100, 4) #(784, 50, 10)-> (2, 5||3, 1), (#입력, #히든 레이어, #출력)\n",
    "iters_num = 10000                 #10000->100\n",
    "train_size = train_x.shape[0] #20\n",
    "learning_rate = 0.01\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = 10\n",
    "for i in range(iters_num):\n",
    "    grad = network.gradient(train_x, train_t)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(train_x, train_t)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(train_x, train_t)\n",
    "        test_acc = network.accuracy(test_x, test_t)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(i, \"train acc:\" + str(train_acc) + \", test acc \" + str(test_acc) + \", loss: \" + str(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
