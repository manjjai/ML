{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ace4423-a1ba-4a5a-a6bc-ddfff56253b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import *\n",
    "from gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10d2108a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'simpleNet' object has no attribute 'W'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m simpleNet()\n\u001b[0;32m     14\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m w: net\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[1;32m---> 15\u001b[0m dW \u001b[38;5;241m=\u001b[39m numerical_gradient(f, \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWeight: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, net\u001b[38;5;241m.\u001b[39mW)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'simpleNet' object has no attribute 'W'"
     ]
    }
   ],
   "source": [
    "class simpleNet:    #간단한 신경망 구현\n",
    "    def init(self):\n",
    "        self.W = np.random.randn(2, 3) #2x3 가중치 배열\n",
    "    def predict(self, x):               #예측값 반환\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)         #z: 예측값\n",
    "        y = softmax(z)              #y = a.f값\n",
    "        loss = cross_entropy_error(y, t)    #교제 제곱 오차: 타깃과 예측의 차이\n",
    "        return loss\n",
    "x = np.array([0.6, 0.9])    #입력값\n",
    "t = np.array([0, 0, 1]) #target: 정답\n",
    "net = simpleNet()\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "for i in range(5):\n",
    "    print(\"\\nWeight: \\n\", net.W)\n",
    "    print(\"activated: \\n\", softmax(net.predict(x)))\n",
    "    print(\"Loss: \\n\", net.loss(x, t))\n",
    "    print(\"=\"*50)\n",
    "    net.W -= dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e0b2817-fc57-44b3-b038-cfc96db6ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weight: \n",
      " [[ 0.17081106  0.00292529  1.95807241 -0.18515059]\n",
      " [ 0.53770961 -1.01906504 -1.34387893  0.03628067]\n",
      " [-0.60927881 -0.03967839 -0.58740372  0.07659958]]\n",
      "activated: \n",
      " [7.50889540e-03 3.93474772e-03 9.88400668e-01 1.55688578e-04]\n",
      "==================================================\n",
      "\n",
      "Weight: \n",
      " [[ 0.16705671  0.50094526  1.46388463 -0.18522843]\n",
      " [ 0.55422876 -3.21035291  0.83054727  0.03662317]\n",
      " [-0.55671788 -7.01195796  6.33122512  0.07768937]]\n",
      "activated: \n",
      " [5.08521712e-24 1.00000000e+00 6.08323673e-45 1.56935039e-25]\n",
      "==================================================\n",
      "\n",
      "Weight: \n",
      " [[  0.16330236   0.99896523   0.96969686  -0.18530627]\n",
      " [  0.57074791  -5.40164077   3.00497348   0.03696568]\n",
      " [ -0.50415694 -13.98423752  13.24985396   0.07877917]]\n",
      "activated: \n",
      " [1.35506410e-47 1.00000000e+00 1.47317146e-91 6.22443558e-49]\n",
      "==================================================\n",
      "\n",
      "Weight: \n",
      " [[  0.15954801   1.4969852    0.47550908  -0.18538411]\n",
      " [  0.58726706  -7.59292864   5.17939969   0.03730819]\n",
      " [ -0.45159601 -20.95651709  20.1684828    0.07986896]]\n",
      "activated: \n",
      " [3.61085606e-071 1.00000000e+000 3.56756484e-138 2.46876661e-072]\n",
      "==================================================\n",
      "\n",
      "Weight: \n",
      " [[ 1.55793654e-01  1.99500517e+00 -1.86786906e-02 -1.85461955e-01]\n",
      " [ 6.03786209e-01 -9.78421650e+00  7.35382590e+00  3.76506933e-02]\n",
      " [-3.99035069e-01 -2.79287967e+01  2.70871116e+01  8.09587517e-02]]\n",
      "activated: \n",
      " [9.62189282e-095 1.00000000e+000 8.63953672e-185 9.79174495e-096]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self, a, b):   #a: X.r, b: X.c\n",
    "        self.W = np.random.randn(a, b)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "    \n",
    "x = np.array([0.5, -2.2, -7])\n",
    "t = np.array([0, 1, 0, 0])\n",
    "net = simpleNet(3, 4)\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "for i in range(5):\n",
    "    print(\"\\nWeight: \\n\", net.W)\n",
    "    print(\"activated: \\n\", softmax(net.predict(x)))\n",
    "    #print(\"Loss: \\n\", net.loss(x, t))\n",
    "    print(\"=\"*50)\n",
    "    net.W -= dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "674531af-1dbb-42be-b411-131ae7de982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): #입력 크기, 히든 레이어 크기, 출력 크기, 실수값으로 조지기\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #입력x히든 사이즈로 난수 배열 만들고 실수로 값 변환\n",
    "        self.params['b1'] = np.zeros(hidden_size)   #히든 레이어 개수만큼 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) #np.sum(y==t) y==t인 case의 횟수를 저장, 이를 평균을 내줌\n",
    "        return accuracy                               #그렇게 구한 정확도를 반환\n",
    "    \n",
    "    def numerical_gradient(self, x, t):                 #손실함수를 w에 대해 미분함 -> gradient를 반환\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        batch_num = x.shape[0]\n",
    "        #forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        #backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea7dd716-7953-4283-bf38-6ed791812ca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasetsub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_mnist\n\u001b[1;32m----> 2\u001b[0m (x_train, y_train), (x_test, t_test) \u001b[38;5;241m=\u001b[39m \u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m iters_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[1;32me:\\Vscode_Jupyter\\인프심\\7주차\\datasetsub\\mnist.py:109\u001b[0m, in \u001b[0;36mload_mnist\u001b[1;34m(normalize, flatten, one_hot_label)\u001b[0m\n\u001b[0;32m    106\u001b[0m     init_mnist()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 109\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_img\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_img\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "from datasetsub.mnist import load_mnist\n",
    "(x_train, y_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "iters_num = 100\n",
    "learning_rate = 1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "for i in range(iters_num):\n",
    "    grad = network.numerical_gradient(x_train, y_train)\n",
    "    for key in {'W1', 'b1', 'W2', 'b2'}:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf969495-be3c-43ff-af2b-f8a30e5686eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (x_train, t_train), (x_test, t_test) \u001b[38;5;241m=\u001b[39m \u001b[43mload_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m network \u001b[38;5;241m=\u001b[39m TwoLayerNet(\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      3\u001b[0m iters_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[1;32me:\\Vscode_Jupyter\\인프심\\7주차\\datasetsub\\mnist.py:109\u001b[0m, in \u001b[0;36mload_mnist\u001b[1;34m(normalize, flatten, one_hot_label)\u001b[0m\n\u001b[0;32m    106\u001b[0m     init_mnist()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 109\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_img\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_img\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(784, 50, 10)\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "iter_per_epoch = 10\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56ed48-6b47-404c-84fc-f964b21add54",
   "metadata": {},
   "source": [
    "### 오늘의 연습\n",
    "### 최소 20개 이상의 자료에 대한 좌표를 지정.\n",
    "### 또한 각 10개 이상의 동일 그룹에 대해 0, 1, ~ 등으로 출력 레이블을 지정\n",
    "### 테스트 좌표는 4개 이상 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67aea16",
   "metadata": {},
   "source": [
    "### 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83dbf4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[0, 0], #입력 4개->20개\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "# OR 게이트\n",
    "t_train = np.array([[0],    #출력 레이블 2개(0, 1) -> \"\"\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [1]])\n",
    "\n",
    "x_test = np.array([[0.5, 0.5], [1.1, 0.9]]) #테스트 좌표 2개 -> 4개\n",
    "t_test = np.array([[1], [1]])               #테스트 좌표의 레이블 2개 -> 2개\n",
    "\n",
    "network = TwoLayerNet(2, 3, 1) #(784, 50, 10)-> (2, 5||3, 1)\n",
    "iters_num = 10000                 #10000->100\n",
    "train_size = x_train.shape[0]\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = 100\n",
    "for i in range(iters_num):\n",
    "    grad = network.gradient(x_train, t_train)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_train, t_train)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d6616",
   "metadata": {},
   "source": [
    "# 과제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): #입력 크기, 히든 레이어 크기, 출력 크기, 실수값으로 조지기\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #입력x히든 사이즈로 난수 배열 만들고 실수로 값 변환\n",
    "        self.params['b1'] = np.zeros(hidden_size)   #히든 레이어 개수만큼 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) #np.sum(y==t) y==t인 case의 횟수를 저장, 이를 평균을 내줌\n",
    "        return accuracy                               #그렇게 구한 정확도를 반환\n",
    "    \n",
    "    def numerical_gradient(self, x, t):                 #손실함수를 w에 대해 미분함 -> gradient를 반환\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):   #x=20x2, t=20x1\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        batch_num = x.shape[0]  #20\n",
    "        #forward\n",
    "        a1 = np.dot(x, W1) + b1 \n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        #backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        return grads\n",
    "\n",
    "\n",
    "#학습데이터 20개 생성\n",
    "x_train0 = np.random.uniform(0, 1, (10, 2))\n",
    "x_train0 = np.round(x_train0, decimals=2)\n",
    "\n",
    "x_train1 = np.random.uniform(1, 2, (10, 2))\n",
    "x_train1 = np.round(x_train1, decimals=2)\n",
    "\n",
    "x_train = np.vstack((x_train0, x_train1))\n",
    "\n",
    "# 출력 레이블: 0 || 1\n",
    "t_train = np.where(np.any(x_train > 1, axis=1), 1, 0).reshape(-1, 1)\n",
    "\n",
    "# 테스트 데이터 4개\n",
    "x_test = np.random.uniform(0, 2, (20, 2))\n",
    "x_test = np.round(x_test, decimals=2)\n",
    "# x_test의 예상 레이블 4개\n",
    "t_test = np.where(np.any(x_test > 1, axis=1), 1, 0).reshape(-1, 1)     \n",
    "\n",
    "network = TwoLayerNet(2, 3, 1) #(784, 50, 10)-> (2, 5||3, 1), (#입력, #히든 레이어, #출력)\n",
    "iters_num = 3000                 #10000->100\n",
    "train_size = x_train.shape[0] #20\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = 100\n",
    "for i in range(iters_num):\n",
    "    grad = network.gradient(x_train, t_train)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_train, t_train)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3021436",
   "metadata": {},
   "source": [
    "## 나도 이거 뭔지 기억 안 남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d21a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# # 20개 이상의 학습 데이터를 생성 (10개씩 두 그룹으로 나눔)\n",
    "# x_train = np.random.uniform(0, 2, (20, 2))\n",
    "# x_train = np.round(x_train, decimals=2)\n",
    "\n",
    "# # 출력 레이블 설정 (그룹 0은 0, 그룹 1은 1로 설정)\n",
    "# t_train = np.where(np.any(x_train > 1, axis=1), 1, 0)   \n",
    "\n",
    "# # 테스트 데이터 (4개 이상의 좌표)\n",
    "# x_test = np.random.uniform(0, 2, (4, 2))\n",
    "# x_test = np.round(x_test, decimals=2)\n",
    "\n",
    "# # 테스트 데이터의 예상 레이블\n",
    "# t_test = np.where(np.any(x_test > 1, axis=1), 1, 0)    \n",
    "\n",
    "\n",
    "# print('x_train')\n",
    "# print(x_train)\n",
    "\n",
    "# print('\\nt_train')\n",
    "# print(t_train)\n",
    "\n",
    "# print('\\nx_test')\n",
    "# print(x_test)\n",
    "\n",
    "# print('\\nt_test')\n",
    "# print(t_test)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
