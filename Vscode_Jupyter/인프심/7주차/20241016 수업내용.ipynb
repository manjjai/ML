{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ace4423-a1ba-4a5a-a6bc-ddfff56253b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import *\n",
    "from gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b96cb4-6df4-4849-8390-c8084e388760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:    #간단한 신경망 구현\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) #2x3 가중치 배열\n",
    "    def predict(self, x):               #예측값 반환\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)         #z: 예측값\n",
    "        y = softmax(z)              #y = a.f값\n",
    "        loss = cross_entropy_error(y, t)    #교제 제곱 오차: 타깃과 예측의 차이\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42754f5c-95aa-4b94-b526-dc247d8770f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.20109725 -1.69259724  0.44337087]\n",
      " [-0.90359848  0.23810579  1.02588161]]\n",
      "[0.19624164 0.09660823 0.70715014]\n",
      "0.34651213772192035\n",
      "[[ 1.08335228 -1.75056217  0.61908076]\n",
      " [-1.08021593  0.1511584   1.28944645]]\n",
      "[0.1259554  0.06967322 0.80437138]\n",
      "0.21769407633254262\n",
      "[[ 0.96560732 -1.8085271   0.79479065]\n",
      " [-1.25683338  0.06421101  1.55301129]]\n",
      "[0.07728408 0.04803584 0.87468008]\n",
      "0.1338969632568107\n",
      "[[ 0.84786235 -1.86649202  0.97050055]\n",
      " [-1.43345083 -0.02273638  1.81657613]]\n",
      "[0.04596438 0.03210132 0.9219343 ]\n",
      "0.08128120919665376\n",
      "[[ 0.73011739 -1.92445695  1.14621044]\n",
      " [-1.61006828 -0.10968377  2.08014098]]\n",
      "[0.02678715 0.02102104 0.95219181]\n",
      "0.048988678335961634\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])    #입력값\n",
    "t = np.array([0, 0, 1]) #target: 정답\n",
    "net = simpleNet()\n",
    "f = lambda w: net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "for i in range(5):\n",
    "    print(net.W)\n",
    "    print(softmax(net.predict(x)))\n",
    "    print(net.loss(x, t))\n",
    "    net.W -= dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0b2817-fc57-44b3-b038-cfc96db6ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36197308  0.51418862  1.61766819  0.81757999]\n",
      " [ 0.75940835 -0.29633415 -0.47506047  1.19658565]\n",
      " [-0.53475504  1.26451112 -0.19537048 -0.35881737]]\n",
      "[2.00717696e-01 1.07564535e-05 7.58892050e-01 4.03794980e-02]\n",
      "11.430750864873882\n",
      "[[-0.46140752  1.00957774  1.24171729  0.79757621]\n",
      " [ 1.19691985 -2.47604626  1.17912351  1.28460228]\n",
      " [ 0.85732705 -5.67093649  5.06794213 -0.07876444]]\n",
      "[2.11314924e-24 1.00000000e+00 8.15143780e-37 2.29252782e-21]\n",
      "-9.999999505838704e-08\n",
      "[[ -0.56084195   1.50496685   0.86576638   0.77757244]\n",
      " [  1.63443135  -4.65575837   2.83330749   1.3726189 ]\n",
      " [  2.24940914 -12.60638411  10.33125475   0.20128848]]\n",
      "[2.39300596e-52 1.00000000e+00 9.41797509e-78 1.40003024e-45]\n",
      "-9.999999505838704e-08\n",
      "[[ -0.66027638   2.00035597   0.48981548   0.75756866]\n",
      " [  2.07194285  -6.83547048   4.48749147   1.46063553]\n",
      " [  3.64149124 -19.54183172  15.59456737   0.48134141]]\n",
      "[2.70992575e-080 1.00000000e+000 1.08813018e-118 8.54988394e-070]\n",
      "-9.999999505838704e-08\n",
      "[[ -0.75971081   2.49574508   0.11386457   0.73756488]\n",
      " [  2.50945435  -9.01518259   6.14167545   1.54865216]\n",
      " [  5.03357333 -26.47727934  20.85787999   0.76139433]]\n",
      "[3.06881708e-108 1.00000000e+000 1.25719943e-159 5.22135260e-094]\n",
      "-9.999999505838704e-08\n"
     ]
    }
   ],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self, a, b):   #a: X.r, b: X.c\n",
    "        self.W = np.random.randn(a, b)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "    \n",
    "x = np.array([0.5, -2.2, -7])\n",
    "t = np.array([0, 1, 0, 0])\n",
    "net = simpleNet(3, 4)\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "for i in range(5):\n",
    "    print(net.W)\n",
    "    print(softmax(net.predict(x)))\n",
    "    print(net.loss(x, t))\n",
    "    net.W -= dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674531af-1dbb-42be-b411-131ae7de982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): #입력 크기, 히든 레이어 크기, 출력 크기, 실수값으로 조지기\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #입력x히든 사이즈로 난수 배열 만들고 실수로 값 변환\n",
    "        self.params['b1'] = np.zeros(hidden_size)   #히든 레이어 개수만큼 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) #np.sum(y==t) y==t인 case의 횟수를 저장, 이를 평균을 내줌\n",
    "        return accuracy                               #그렇게 구한 정확도를 반환\n",
    "    \n",
    "    def numerical_gradient(self, x, t):                 #손실함수를 w에 대해 미분함 -> gradient를 반환\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        batch_num = x.shape[0]\n",
    "        #forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        #backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7dd716-7953-4283-bf38-6ed791812ca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m network \u001b[38;5;241m=\u001b[39m TwoLayerNet(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters_num):\n\u001b[1;32m----> 8\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[0;32m     10\u001b[0m         network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grad[key]\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     27\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     28\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 29\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     31\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32me:\\Vscode_Jupyter\\인프심\\7주차\\gradient.py:43\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):                 \u001b[38;5;66;03m#손실함수를 w에 대해 미분함 -> gradient를 반환\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     29\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 17\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[0;32m     12\u001b[0m z1 \u001b[38;5;241m=\u001b[39m sigmoid(a1)\n\u001b[0;32m     13\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, W2) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasetsub.mnist import load_mnist\n",
    "(x_train, y_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "iters_num = 100\n",
    "learning_rate = 1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "for i in range(iters_num):\n",
    "    grad = network.numerical_gradient(x_train, y_train)\n",
    "    for key in {'W1', 'b1', 'W2', 'b2'}:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf969495-be3c-43ff-af2b-f8a30e5686eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(784, 50, 10)\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "iter_per_epoch = 10\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56ed48-6b47-404c-84fc-f964b21add54",
   "metadata": {},
   "source": [
    "### 오늘의 연습\n",
    "### 최소 20개 이상의 자료에 대한 좌표를 지정.\n",
    "### 또한 각 10개 이상의 동일 그룹에 대해 0, 1, ~ 등으로 출력 레이블을 지정\n",
    "### 테스트 좌표는 4개 이상 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67aea16",
   "metadata": {},
   "source": [
    "### 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83dbf4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n",
      "train acc, test acc | 1.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[0, 0], #입력 4개->20개\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "# OR 게이트\n",
    "t_train = np.array([[0],    #출력 레이블 2개(0, 1) -> \"\"\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [1]])\n",
    "\n",
    "x_test = np.array([[0.5, 0.5], [1.1, 0.9]]) #테스트 좌표 2개 -> 4개\n",
    "t_test = np.array([[1], [1]])               #테스트 좌표의 레이블 2개 -> 2개\n",
    "\n",
    "network = TwoLayerNet(2, 3, 1) #(784, 50, 10)-> (2, 5||3, 1)\n",
    "iters_num = 10000                 #10000->100\n",
    "train_size = x_train.shape[0]\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = 100\n",
    "for i in range(iters_num):\n",
    "    grad = network.gradient(x_train, t_train)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_train, t_train)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d6616",
   "metadata": {},
   "source": [
    "# 과제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): #입력 크기, 히든 레이어 크기, 출력 크기, 실수값으로 조지기\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #입력x히든 사이즈로 난수 배열 만들고 실수로 값 변환\n",
    "        self.params['b1'] = np.zeros(hidden_size)   #히든 레이어 개수만큼 \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) #np.sum(y==t) y==t인 case의 횟수를 저장, 이를 평균을 내줌\n",
    "        return accuracy                               #그렇게 구한 정확도를 반환\n",
    "    \n",
    "    def numerical_gradient(self, x, t):                 #손실함수를 w에 대해 미분함 -> gradient를 반환\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):   #x=20x2, t=20x1\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        batch_num = x.shape[0]  #20\n",
    "        #forward\n",
    "        a1 = np.dot(x, W1) + b1 \n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        #backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        return grads\n",
    "\n",
    "\n",
    "#학습데이터 20개 생성\n",
    "x_train0 = np.random.uniform(0, 1, (10, 2))\n",
    "x_train0 = np.round(x_train0, decimals=2)\n",
    "\n",
    "x_train1 = np.random.uniform(1, 2, (10, 2))\n",
    "x_train1 = np.round(x_train1, decimals=2)\n",
    "\n",
    "x_train = np.vstack((x_train0, x_train1))\n",
    "\n",
    "# 출력 레이블: 0 || 1\n",
    "t_train = np.where(np.any(x_train > 1, axis=1), 1, 0).reshape(-1, 1)\n",
    "\n",
    "# 테스트 데이터 4개\n",
    "x_test = np.random.uniform(0, 2, (20, 2))\n",
    "x_test = np.round(x_test, decimals=2)\n",
    "# x_test의 예상 레이블 4개\n",
    "t_test = np.where(np.any(x_test > 1, axis=1), 1, 0).reshape(-1, 1)     \n",
    "\n",
    "network = TwoLayerNet(2, 3, 1) #(784, 50, 10)-> (2, 5||3, 1), (#입력, #히든 레이어, #출력)\n",
    "iters_num = 3000                 #10000->100\n",
    "train_size = x_train.shape[0] #20\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = 100\n",
    "for i in range(iters_num):\n",
    "    grad = network.gradient(x_train, t_train)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    loss = network.loss(x_train, t_train)\n",
    "    train_loss_list.append(loss)\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3021436",
   "metadata": {},
   "source": [
    "## 나도 이거 뭔지 기억 안 남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d21a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# # 20개 이상의 학습 데이터를 생성 (10개씩 두 그룹으로 나눔)\n",
    "# x_train = np.random.uniform(0, 2, (20, 2))\n",
    "# x_train = np.round(x_train, decimals=2)\n",
    "\n",
    "# # 출력 레이블 설정 (그룹 0은 0, 그룹 1은 1로 설정)\n",
    "# t_train = np.where(np.any(x_train > 1, axis=1), 1, 0)   \n",
    "\n",
    "# # 테스트 데이터 (4개 이상의 좌표)\n",
    "# x_test = np.random.uniform(0, 2, (4, 2))\n",
    "# x_test = np.round(x_test, decimals=2)\n",
    "\n",
    "# # 테스트 데이터의 예상 레이블\n",
    "# t_test = np.where(np.any(x_test > 1, axis=1), 1, 0)    \n",
    "\n",
    "\n",
    "# print('x_train')\n",
    "# print(x_train)\n",
    "\n",
    "# print('\\nt_train')\n",
    "# print(t_train)\n",
    "\n",
    "# print('\\nx_test')\n",
    "# print(x_test)\n",
    "\n",
    "# print('\\nt_test')\n",
    "# print(t_test)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
